{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your Libraries\n",
    "All the libraries listed below are required to run this notebook.  \n",
    "\n",
    "If you require a GPU to train your model (for example, you are training a deep learning model), use DEFAULT_GPU_IMAGE instead of DEFAULT_CPU_IMAGE.  \n",
    "\n",
    "You can also modify the Hyperparameter run with some of the optional functions following this documentation: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "# Load Azure libaries\n",
    "import azureml.core\n",
    "from azureml.core import Datastore, Dataset, Environment, Experiment, Model, ScriptRunConfig, Workspace\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE, RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineEndpoint, PipelineParameter, PipelineRun\n",
    "from azureml.pipeline.core import PublishedPipeline, StepSequence, TrainingOutput\n",
    "from azureml.pipeline.steps import PythonScriptStep, HyperDriveStep\n",
    "from azureml.train.hyperdrive import HyperDriveRun, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import BayesianParameterSampling, uniform, choice\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Modify this workbook with some of the optional Azure libraries below\n",
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "from azureml.train.hyperdrive import normal, GridParameterSampling, RandomParameterSampling\n",
    "from azureml.train.hyperdrive import BanditPolicy, MedianStoppingPolicy, TruncationSelectionPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect your Workspace\n",
    "When using an Azure Notebook, you must first connect it to your Azure Machine Learning Service to access objects within the Workspace.  \n",
    "\n",
    "Use the code below and follow the instructions to sign in.  \n",
    "\n",
    "Also, issues may arise if you are use a different version of the Azure ML SDK.  If you encounter errors, <b>install the version this notebook was created with</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are currently using version 1.31.0 of the Azure ML SDK\n",
      "This notebook was made using version 1.31.0 of the Azure ML SDK\n"
     ]
    }
   ],
   "source": [
    "# Check which version of the AzureML SDK you are using\n",
    "print(\"You are currently using version \" + azureml.core.VERSION + \" of the Azure ML SDK\")\n",
    "print(\"This notebook was made using version 1.31.0 of the Azure ML SDK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect your Jupyter Notebook Server to your AMLS Workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your Remote Compute Target\n",
    "\n",
    "When you submit this run, it will run on a cluster of virtual machines.  Specify the cluster below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Compute Cluster for running your Pipeline jobs remotely\n",
    "computeName = 'automl-cluster'  # CHANGE THIS\n",
    "computeTarget = ComputeTarget(ws, computeName) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Environment which contains all the libraries needed for your scripts\n",
    "When you submit this run, it will create a docker container using all of the packages you list in this object.\n",
    "\n",
    "If a package is available through both conda and pip, <b>use the conda version</b>, as conda automatically reconciles package discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out which packages are available in Conda, uncomment and run the code below\n",
    "#%conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give your environment a name\n",
    "environment = Environment(name=\"XGBoostTrainingEnv\") # CHANGE HERE\n",
    "condaDep = CondaDependencies()\n",
    "\n",
    "# Add conda packages\n",
    "# CHANGE HERE TO MATCH SCRIPT\n",
    "condaDep.add_conda_package(\"scikit-learn==0.22.1\")\n",
    "condaDep.add_conda_package(\"numpy==1.16.2\")\n",
    "condaDep.add_conda_package(\"matplotlib==3.2.1\")\n",
    "condaDep.add_conda_package(\"joblib==0.14.1\")\n",
    "condaDep.add_conda_package(\"xgboost==0.90\")\n",
    "condaDep.add_conda_package(\"seaborn==0.9.0\")\n",
    "condaDep.add_conda_package(\"pandas==0.23.4\")\n",
    "condaDep.add_conda_package(\"scipy==1.3.1\")\n",
    "\n",
    "# Add pip packages\n",
    "# CHANGE HERE TO MATCH SCRIPT\n",
    "condaDep.add_pip_package(\"azureml-defaults==1.31.0\")\n",
    "condaDep.add_pip_package(\"azureml-interpret==1.31.0\")\n",
    "condaDep.add_pip_package(\"azureml-explain-model==1.31.0\")\n",
    "condaDep.add_pip_package(\"pyarrow==1.0.1\")\n",
    "condaDep.add_pip_package(\"pytz==2021.1\")\n",
    "condaDep.add_pip_package(\"interpret-core==0.1.21\")\n",
    "condaDep.add_pip_package(\"lightgbm==2.3.0\")\n",
    "\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "environment.python.conda_dependencies=condaDep\n",
    "\n",
    "# Register the environment to your workspace\n",
    "trainingEnvironment = environment.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Run Configuration object to dockerize your environment\n",
    "runConfig = RunConfiguration()\n",
    "runConfig.docker.use_docker = True\n",
    "runConfig.environment = environment\n",
    "runConfig.environment.docker.base_image = DEFAULT_CPU_IMAGE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Registration, Training, Model Registration, and Metrics Output Scripts for your Pipeline\n",
    "When you run this pipeline, it will run a series of .py scripts.  Specify the folder name and file names of your scripts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder on your Jupyter Notebook server to store your .py files.\n",
    "projectFolder = './XGB_Pipeline_Scripts/Training'\n",
    "os.makedirs(projectFolder, exist_ok=True)\n",
    "\n",
    "# Assign a file name for your .py file which will perform unit test.\n",
    "unitTestingFileName = 'XGB_Hyperdrive_Unit_Testing.py'\n",
    "\n",
    "# Assign a file name for your .py file which will contain your shared functions.\n",
    "sharedFunctionsFileName = 'XGB_Hyperdrive_Shared_Functions.py'\n",
    "\n",
    "# Assign a file name for your .py file which will register your model.\n",
    "datasetRegistrationFileName = 'XGB_Hyperdrive_Dataset_Registration.py'\n",
    "\n",
    "# Assign a file name for your .py file which will execute your training script.\n",
    "trainingFileName = 'XGB_Hyperdrive_Training.py'\n",
    "\n",
    "# Assign a file name for your .py file which will register your model.\n",
    "modelRegistrationFileName = 'XGB_Hyperdrive_Model_Registration.py'\n",
    "\n",
    "# Assign a file name for your .py file which will output your Hyperdrive Metrics.\n",
    "metricsOutputFileName = 'XGB_Hyperdrive_Metrics_Output.py'\n",
    "\n",
    "# Create file path strings\n",
    "sharedFunctionsFilePath = os.path.join(projectFolder, sharedFunctionsFileName)\n",
    "unitTestingFilePath = os.path.join(projectFolder, unitTestingFileName)\n",
    "datasetRegistrationFilePath = os.path.join(projectFolder, datasetRegistrationFileName)\n",
    "trainingFilePath = os.path.join(projectFolder, trainingFileName)\n",
    "modelRegistrationFilePath = os.path.join(projectFolder, modelRegistrationFileName)\n",
    "metricsOutputFilePath = os.path.join(projectFolder, metricsOutputFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Shared Functions Script\n",
    "This script holds functions which you use across multiple .py files in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./XGB_Pipeline_Scripts/Training/XGB_Hyperdrive_Shared_Functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $sharedFunctionsFilePath\n",
    "# Load in libaries\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import zip_longest\n",
    "\n",
    "# Creates a Python Dictionary object out of key-value pairs\n",
    "def create_dict(keys, values):\n",
    "    return dict(zip_longest(keys, values[:len(keys)]))\n",
    "\n",
    "# Sets tags for Azure resources\n",
    "def set_tags(tagNameList, tagValueList):\n",
    "    return create_dict(tagNameList, tagValueList)\n",
    "\n",
    "# Writes CSV files back to a storage account\n",
    "def write_to_datastore(dataframe, workspace, datastore, folder, file, indexBoolean):\n",
    "    os.makedirs(folder, exist_ok=True) \n",
    "    filePath = os.path.join(folder, file)\n",
    "    dataframe.to_csv(filePath, index = indexBoolean)\n",
    "    print('Data Written as CSV and saved to ' + filePath)\n",
    "    \n",
    "    # Upload to Datastore\n",
    "    files = [filePath]\n",
    "    datastore.upload_files(files=files, target_path=folder, overwrite = True)\n",
    "    \n",
    "# Split data into X (features) and Y (target) columns for machine learning\n",
    "def split_x_y (dataframe, scoring_column):\n",
    "    X = dataframe.drop(scoring_column, axis=1)\n",
    "    Y = dataframe[scoring_column]\n",
    "    return X, Y\n",
    "    \n",
    "# Make predictions using a classification model\n",
    "def make_classification_predictions(model, dataframe, X, Y):\n",
    "    dataframe.loc[:, 'Predictions'] = model.predict(X)\n",
    "    for i in range(0, len(Y.unique())):\n",
    "        dataframe.loc[:, 'Probability_' + str(Y.unique()[i])] = model.predict_proba(X)[:,i]\n",
    "    return dataframe\n",
    "\n",
    "# Saves local explanations as columns to a dataframe\n",
    "def save_local_explanations(explainerModel, dataframe, features):\n",
    "    localExplanation = explainerModel.explain_local(features)\n",
    "    localImportanceNames = localExplanation.get_ranked_local_names()\n",
    "    localImportanceValues = localExplanation.get_ranked_local_values()\n",
    "    dataframe['ExplanationColumns'] = localImportanceNames[0]\n",
    "    dataframe['ExplanationValues'] = localImportanceValues[0]\n",
    "    dataframe['Explanations'] = 'fill'\n",
    "    for i in range(0,len(dataframe)):\n",
    "        dataframe['Explanations'][i] = json.dumps(dict(zip(dataframe.ExplanationColumns[i],\\\n",
    "                                                           dataframe.ExplanationValues[i])))\n",
    "    dataframe = dataframe.drop(['ExplanationColumns','ExplanationValues'], axis=1)\n",
    "    return dataframe\n",
    "\n",
    "# Saves local explanations as columns to a dataframe\n",
    "def save_global_explanations(explainerModel, features):\n",
    "    globalExplanation = explainerModel.explain_global(features)\n",
    "    pd.DataFrame()\n",
    "    global_names = globalExplanation.get_ranked_global_names()\n",
    "    global_values = globalExplanation.get_ranked_global_values()\n",
    "    global_zipped = list(zip(global_names, global_values))\n",
    "    dataframe = pd.DataFrame(global_zipped, columns=['Columns','Importance'])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Unit Testing Script\n",
    "Use this script to perform unit tests for your functions.  If any fail, the whole pipeline will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./XGB_Pipeline_Scripts/Training/XGB_Hyperdrive_Unit_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $unitTestingFilePath\n",
    "# Load in libaries\n",
    "import unittest\n",
    "\n",
    "# Load in functions from your other .py files to unit test\n",
    "from XGB_Hyperdrive_Shared_Functions import create_dict, set_tags\n",
    "\n",
    "class TestFunctions(unittest.TestCase):\n",
    "      \n",
    "    def setUp(self):\n",
    "        pass\n",
    "  \n",
    "    # Returns True if the output is a dictionary and matches the correct format\n",
    "    def test_create_dict(self):\n",
    "        self.assertEqual(create_dict(['Key1', 'Key2'], [123, 456]), {'Key1': 123, 'Key2': 456})\n",
    "  \n",
    "    # Returns True if the output is a dictionary and matches the correct format\n",
    "    def test_set_tags(self):        \n",
    "        self.assertEqual(['Project', 'Message'], {'Project': 'Test', 'Message': 'Testing'})\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    print('Script Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Dataset Registration Script\n",
    "This script registers data on your datastore as datasets.  It will register two datasets, one for training data and the other for validation data. \n",
    "\n",
    "It expects data to be located in your datastore in the following format, \"projectFolder/trainingInputFolder/2021-07-22/yourFile\"\n",
    "\n",
    "Feel free to name the folders and files whatever you wish, but the date folder must be <b>today's date</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./XGB_Pipeline_Scripts/Training/XGB_Hyperdrive_Dataset_Registration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $datasetRegistrationFilePath\n",
    "# Load in libaries\n",
    "import argparse\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from itertools import zip_longest\n",
    "\n",
    "# Load in Azure libraries\n",
    "from azureml.core import Dataset, Datastore, Run, Workspace\n",
    "\n",
    "# Load in functions from shared functions file\n",
    "from XGB_Hyperdrive_Shared_Functions import create_dict, set_tags, write_to_datastore\n",
    "\n",
    "# Define script-specific functions\n",
    "def register_dataset(workspace, datastore, folder, file, datasetName, description, tags):\n",
    "    filePath = os.path.join(folder, file)\n",
    "    datastorePath = [(datastore, filePath)]\n",
    "    dataset = Dataset.Tabular.from_delimited_files(datastorePath)\n",
    "    dataset.register(workspace = workspace, \n",
    "                     create_new_version = True,\n",
    "                     name = datasetName,\n",
    "                     description = description,\n",
    "                     tags = tags)\n",
    "    \n",
    "def init():\n",
    "    # Set Arguments\n",
    "    global args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_dataset_name', type=str,\n",
    "                            help='Name of Training Dataset')\n",
    "    parser.add_argument('--val_dataset_name', type=str,\n",
    "                            help='Name of Validation Dataset')\n",
    "    parser.add_argument('--datastore_path', type=str,\n",
    "                            help='Location of file or files on Datastore')\n",
    "    parser.add_argument('--datastore_name', type=str,\n",
    "                            help='Name of Datastore')\n",
    "    parser.add_argument('--train_file_name', type=str,\n",
    "                            help='Name of training data file on Datastore')\n",
    "    parser.add_argument('--val_file_name', type=str,\n",
    "                            help='Name of validation data file on Datastore')\n",
    "    parser.add_argument('--project_name', type=str,\n",
    "                            help='Name of project')\n",
    "    parser.add_argument('--project_description', type=str,\n",
    "                            help='Description of project')\n",
    "    parser.add_argument('--pytz_time_zone', type=str,\n",
    "                            help='Time Zone associated with your data')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set the Run context for logging\n",
    "    global run\n",
    "    run = Run.get_context()\n",
    "\n",
    "def main():\n",
    "    # Connect to your AMLS Workspace and set your Datastore\n",
    "    ws = run.experiment.workspace\n",
    "    datastoreName = args.datastore_name\n",
    "    datastore = Datastore.get(ws, datastoreName)\n",
    "    print('Datastore Set')\n",
    "    \n",
    "    # Set your Time Zone\n",
    "    timeZone = pytz.timezone(args.pytz_time_zone)\n",
    "    timeLocal = dt.datetime.now(timeZone).strftime('%Y-%m-%d')\n",
    "    print('Time Zone Set')\n",
    "\n",
    "    # Specify your File Names\n",
    "    trainFile = timeLocal + '/' + args.train_file_name\n",
    "    valFile = timeLocal + '/' + args.val_file_name\n",
    "    print('File Names Set for Training and Validation Data.')\n",
    "    \n",
    "    # Set Tags and Description\n",
    "    description = args.project_description\n",
    "    trainTags = set_tags(['Project', 'Dataset Type', 'Date Created'],\\\n",
    "                         [args.project_name, 'Training', timeLocal])\n",
    "    valTags = set_tags(['Project', 'Dataset Type', 'Date Created'],\\\n",
    "                       [args.project_name, 'Validation', timeLocal])\n",
    "    print(\"Dataset Tags and Description Assigned\")\n",
    "    \n",
    "    # Register your Training data as an Azure Tabular Dataset\n",
    "    register_dataset(ws, datastore, args.datastore_path, trainFile, args.train_dataset_name, description, trainTags)\n",
    "    print('Training Data Registered')\n",
    "    \n",
    "    # Register your Validation data as an Azure Tabular Dataset\n",
    "    register_dataset(ws, datastore, args.datastore_path, valFile, args.val_dataset_name, description, valTags)\n",
    "    print('Validation Data Registered')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()\n",
    "    print('Script Initialized')\n",
    "    main()\n",
    "    print('Script Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your XGBoost Model Training Script\n",
    "This script will train and hypeparameter tune your model.  \n",
    "\n",
    "It will output models, logs, charts, and metrics for all runs, along with predictions and explanations for your validation dataset.\n",
    "\n",
    "Feel free to add in <b>custom metrics</b> or charts into the scoring and charting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./XGB_Pipeline_Scripts/Training/XGB_Hyperdrive_Training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $trainingFilePath\n",
    "# Load in Libraries\n",
    "import argparse\n",
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import LGBMExplainableModel\n",
    "from itertools import zip_longest\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, auc, precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve\n",
    "from sklearn.model_selection import ShuffleSplit, cross_validate\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load in Azure libraries\n",
    "from azureml.core import Run, Dataset, Workspace, Experiment\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "# Load in functions from shared functions file\n",
    "from XGB_Hyperdrive_Shared_Functions import create_dict, save_local_explanations, make_classification_predictions\n",
    "from XGB_Hyperdrive_Shared_Functions import split_x_y, save_global_explanations\n",
    "\n",
    "# Define script-specific functions\n",
    "def score_log_classification_training_data(model, features, target_column, cv_splits, bootstrap_sample_number):\n",
    "    metrics_cv = cross_validate(model, features, target_column,\\\n",
    "                                scoring=[\"accuracy\", \"balanced_accuracy\", \"precision\", \"recall\", \"f1\",], cv=cv_splits)\n",
    "    \n",
    "    # Get average of each metric across cross validation splits\n",
    "    accuracy = np.mean(metrics_cv['test_accuracy'])\n",
    "    balanced_accuracy = np.mean(metrics_cv['test_balanced_accuracy'])\n",
    "    precision = np.mean(metrics_cv['test_precision'])\n",
    "    recall = np.mean(metrics_cv['test_recall'])\n",
    "    F1 = np.mean(metrics_cv['test_f1'])\n",
    "    \n",
    "    # Calculate Confidence Intervals for each of the metrics via bootstrapping cross-validated means\n",
    "    resampled_mean_accuracy = []\n",
    "    resampled_mean_balanced_accuracy = []\n",
    "    resampled_mean_precision = []\n",
    "    resampled_mean_recall = []\n",
    "    resampled_mean_F1 = []\n",
    "    metricsDF = pd.DataFrame(metrics_cv)\n",
    "    \n",
    "    for i in range(0, bootstrap_sample_number):\n",
    "        resample = metricsDF.sample(frac=1, replace=True)\n",
    "        mean_accuracy = np.mean(resample['test_accuracy'])\n",
    "        mean_balanced_accuracy = np.mean(resample['test_balanced_accuracy'])\n",
    "        mean_precision = np.mean(resample['test_precision'])\n",
    "        mean_recall = np.mean(resample['test_recall'])\n",
    "        mean_F1 = np.mean(resample['test_f1'])\n",
    "        resampled_mean_accuracy.append(mean_accuracy)\n",
    "        resampled_mean_balanced_accuracy.append(mean_balanced_accuracy)\n",
    "        resampled_mean_precision.append(mean_precision)\n",
    "        resampled_mean_recall.append(mean_recall)\n",
    "        resampled_mean_F1.append(mean_F1)\n",
    "    resampled_mean_accuracy.sort()\n",
    "    resampled_mean_balanced_accuracy.sort()\n",
    "    resampled_mean_precision.sort()\n",
    "    resampled_mean_recall.sort()\n",
    "    resampled_mean_F1.sort()\n",
    "    lower_bound_index = int(np.floor(bootstrap_sample_number*(1-args.confidence_level)/2))\n",
    "    upper_bound_index = int(np.floor(bootstrap_sample_number*(1+args.confidence_level)/2))\n",
    "    \n",
    "    print(\"Scoring Done for Training Data\")\n",
    "    \n",
    "    # Log training metrics\n",
    "    run.log('Accuracy Training', np.float(accuracy))\n",
    "    run.log('Balanced Accuracy Training', np.float(balanced_accuracy))\n",
    "    run.log('Recall Training', np.float(recall))\n",
    "    run.log('Precision Training', np.float(precision))\n",
    "    run.log('F1 Training', np.float(F1))\n",
    "    run.log('Accuracy Training Lower CI', np.float(resampled_mean_accuracy[lower_bound_index]))\n",
    "    run.log('Balanced Accuracy Training Lower CI', np.float(resampled_mean_balanced_accuracy[lower_bound_index]))\n",
    "    run.log('Recall Training Lower CI', np.float(resampled_mean_recall[lower_bound_index]))\n",
    "    run.log('Precision Training Lower CI', np.float(resampled_mean_precision[lower_bound_index]))\n",
    "    run.log('F1 Training Lower CI', np.float(resampled_mean_F1[lower_bound_index]))\n",
    "    run.log('Accuracy Training Upper CI', np.float(resampled_mean_accuracy[upper_bound_index]))\n",
    "    run.log('Balanced Accuracy Training Upper CI', np.float(resampled_mean_balanced_accuracy[upper_bound_index]))\n",
    "    run.log('Recall Training Upper CI', np.float(resampled_mean_recall[upper_bound_index]))\n",
    "    run.log('Precision Training Upper CI', np.float(resampled_mean_recall[upper_bound_index]))\n",
    "    run.log('F1 Training Upper CI', np.float(resampled_mean_F1[upper_bound_index]))\n",
    "    run.log_list('Accuracy for all CV Splits', metrics_cv['test_accuracy'])\n",
    "    run.log_list('Balanced Accuracy for all CV Splits', metrics_cv['test_balanced_accuracy'])\n",
    "    run.log_list('Precision for all CV Splits', metrics_cv['test_precision'])\n",
    "    run.log_list('Recall for all CV Splits', metrics_cv['test_recall'])\n",
    "    run.log_list('F1 for all CV Splits', metrics_cv['test_f1'])\n",
    "    return print(\"Metrics Logged for Training Data\")\n",
    "    \n",
    "def score_log_classification_validation_data(classificationModel, features, target_column):\n",
    "    val_binary_predictions = classificationModel.predict(features)\n",
    "    val_accuracy = accuracy_score(target_column, val_binary_predictions)\n",
    "    val_balanced_accuracy = balanced_accuracy_score(target_column, val_binary_predictions)\n",
    "    val_precision = precision_score(target_column, val_binary_predictions)\n",
    "    val_recall = recall_score(target_column, val_binary_predictions)\n",
    "    val_F1 = f1_score(target_column, val_binary_predictions)\n",
    "    print(\"Scoring Done for Validation Data\")\n",
    "    \n",
    "    run.log('Accuracy Validation', np.float(val_accuracy))\n",
    "    run.log('Balanced Accuracy Validation', np.float(val_balanced_accuracy))\n",
    "    run.log('Recall Validation', np.float(val_recall))\n",
    "    run.log('Precision Validation', np.float(val_precision))\n",
    "    run.log('F1 Validation', np.float(val_F1))\n",
    "    return print(\"Metrics Logged for Validation Data\")\n",
    "\n",
    "def log_classification_charts(dataType, classificationModel, features, targetColumn):\n",
    "    # Get predictions and actuals to make Confusion Matrix and Precision Recall Curve\n",
    "    binary_predictions = classificationModel.predict(features)\n",
    "    probability_predictions = classificationModel.predict_proba(features)[:,1]\n",
    "    print(\"Predictions Made for \" + dataType + \" Data Charts\")\n",
    "    \n",
    "    # Log a Confusion Matrix\n",
    "    data = pd.DataFrame(dict(s1 = targetColumn, s2 = binary_predictions)).reset_index()\n",
    "    confusion_matrix = pd.crosstab(data['s1'], data['s2'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    fig = plt.figure()\n",
    "    sns.heatmap(confusion_matrix, annot=True,cmap='Blues', fmt='g')\n",
    "    plt.title(\"Confusion Matrix \" + dataType)\n",
    "    plt.close(fig)\n",
    "    run.log_image(name='confusion-matrix-' + dataType.lower(), plot=fig)\n",
    "    print(\"Confusion Matrix Logged for \" + dataType + \" Data\")\n",
    "\n",
    "    # Log a Precision / Recall Curve\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(targetColumn, probability_predictions)\n",
    "    lr_f1, lr_auc = f1_score(targetColumn, binary_predictions), auc(lr_recall, lr_precision)\n",
    "    no_skill = len(targetColumn[targetColumn==1]) / len(targetColumn)\n",
    "    fig2 = plt.figure()\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    plt.title('Precision Recall Curve ' + dataType)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.close(fig2)\n",
    "    run.log_image(name='precision-recall-curve-' + dataType.lower(), plot=fig2)\n",
    "    print(\"Precision Recall Curve Logged for \" + dataType + \" Data\")\n",
    "    \n",
    "    # Log a Receiving Operating Characteristic (ROC) Curve  \n",
    "    fpr, tpr, thresholds = roc_curve(targetColumn, probability_predictions) \n",
    "    fig3 = plt.figure()\n",
    "    plt.plot(fpr, tpr, color='lightblue', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve ' + dataType)\n",
    "    plt.legend()\n",
    "    plt.close(fig3)\n",
    "    run.log_image(name='roc-curve-' + dataType.lower(), plot=fig3)\n",
    "    print(\"ROC Curve Logged for \" + dataType + \" Data\")\n",
    "    \n",
    "def init():\n",
    "    # Set Arguments.  These should be all of the hyperparameters you will tune.\n",
    "    global args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Hyperparameters\n",
    "    parser.add_argument('--eta', type=float, default=0.1,\n",
    "                        help='Learning Rate')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1,\n",
    "                        help='Learning Rate')\n",
    "    parser.add_argument('--scale_pos_weight', type=float, default=0.6,\n",
    "                        help='Helps with Unbalanced Classes.  Should be Sum(Negative)/Sum(Positive)')\n",
    "    parser.add_argument('--booster', type=str, default='gbtree',\n",
    "                        help='The type of Boosting Algorithim')\n",
    "    parser.add_argument('--min_child_weight', type=float, default=1,\n",
    "                        help='Controls Overfitting')\n",
    "    parser.add_argument('--max_depth', type=int, default=6,\n",
    "                        help='Controls Overfitting')\n",
    "    parser.add_argument('--gamma', type=float, default=0,\n",
    "                        help='Make Algorithm Conservative')\n",
    "    parser.add_argument('--subsample', type=float, default=1,\n",
    "                        help='Controls Overfitting')\n",
    "    parser.add_argument('--colsample_bytree', type=float, default=1,\n",
    "                        help='Defines Sampling')\n",
    "    parser.add_argument('--reg_lambda', type=float, default=1,\n",
    "                        help='Controls Overfitting')\n",
    "    parser.add_argument('--alpha', type=float, default=0,\n",
    "                        help='Reduces Dimensionality')\n",
    "    parser.add_argument('--objective', type=str, default='binary:logistic',\n",
    "                        help='Defines Training Objective Metric')\n",
    "    # Other Parameters\n",
    "    parser.add_argument('--train_dataset_name', type=str,\n",
    "                        help='Name of Training Dataset')\n",
    "    parser.add_argument('--val_dataset_name', type=str,\n",
    "                        help='Name of Validation Dataset')\n",
    "    parser.add_argument('--target_column_name', type=str,\n",
    "                        help='Name of variable to score')\n",
    "    parser.add_argument('--k_folds', type=int, default = 10,\n",
    "                        help='Number of folds to split your data into for cross validation')\n",
    "    parser.add_argument('--shuffle_split_size', type=float,\n",
    "                        help='Percentage of data to hold out for testing during cross validation')\n",
    "    parser.add_argument('--confidence_level', type=float, default = 0.95,\n",
    "                        help='Level of confidence to set for your confidence interval ()')\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    # Set the Run context for logging\n",
    "    global run\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    # log your hyperparameters,\n",
    "    run.log('eta',np.float(args.eta))\n",
    "    run.log('learning_rate',np.float(args.learning_rate))\n",
    "    run.log('scale_pos_weight',np.float(args.scale_pos_weight))\n",
    "    run.log('booster',np.str(args.booster))\n",
    "    run.log('min_child_weight',np.float(args.min_child_weight))\n",
    "    run.log('max_depth',np.float(args.max_depth))\n",
    "    run.log('gamma',np.float(args.gamma))\n",
    "    run.log('subsample',np.float(args.subsample))\n",
    "    run.log('colsample_bytree',np.float(args.colsample_bytree))\n",
    "    run.log('reg_lambda',np.float(args.reg_lambda))\n",
    "    run.log('alpha',np.float(args.alpha))\n",
    "    run.log('objective',np.str(args.objective))\n",
    "\n",
    "# Write your main function.  This will train and log your model.\n",
    "def main():\n",
    "    # Connect to your AMLS Workspace and retrieve your data\n",
    "    ws = run.experiment.workspace\n",
    "    training_dataset_name  = args.train_dataset_name\n",
    "    train_dataset  = Dataset.get_by_name(ws, training_dataset_name, version='latest')\n",
    "    val_dataset_name  = args.val_dataset_name\n",
    "    val_dataset  = Dataset.get_by_name(ws, val_dataset_name, version='latest')\n",
    "    print('Datasets Retrieved')\n",
    "    \n",
    "    # Transform your data to Pandas\n",
    "    trainTab =  train_dataset\n",
    "    trainDF = trainTab.to_pandas_dataframe()\n",
    "    valTab =  val_dataset\n",
    "    valDF = valTab.to_pandas_dataframe()\n",
    "    print('Datasets Converted to Pandas')\n",
    "    \n",
    "    # Split out X and Y variables for both training and validation data\n",
    "    X, Y = split_x_y(trainDF, args.target_column_name)\n",
    "    val_X, val_Y = split_x_y(valDF, args.target_column_name)\n",
    "    print(\"Data Ready for Scoring\")\n",
    " \n",
    "    # Set your model and hyperparameters\n",
    "    hyperparameters = dict(eta=args.eta,\\\n",
    "                           learning_rate=args.learning_rate,\\\n",
    "                           scale_pos_weight=args.scale_pos_weight,\\\n",
    "                           booster = args.booster,\\\n",
    "                           min_child_weight = args.min_child_weight,\\\n",
    "                           max_depth = args.max_depth,\\\n",
    "                           gamma = args.gamma,\\\n",
    "                           subsample = args.subsample,\\\n",
    "                           colsample_bytree = args.colsample_bytree,\\\n",
    "                           reg_lambda = args.reg_lambda,\\\n",
    "                           alpha = args.alpha,\\\n",
    "                           objective = args.objective)\n",
    "    \n",
    "    model = XGBClassifier(**hyperparameters)\n",
    "    print('Hyperparameters Set')\n",
    "    \n",
    "    # Fit your model\n",
    "    xgbModel = model.fit(X,Y)\n",
    "    print(\"Model Fit\")\n",
    "    \n",
    "    # Score your training data with cross validation and log metrics\n",
    "    ss = ShuffleSplit(n_splits=args.k_folds, test_size=args.shuffle_split_size, random_state = 33)\n",
    "    bootstrap_sample_number = args.k_folds*100\n",
    "    score_log_classification_training_data(model, X, Y, ss, bootstrap_sample_number)\n",
    "    \n",
    "    # Log a Confusion Matrix and Precision Recall Curve for your training data\n",
    "    log_classification_charts(\"Training\", xgbModel, X, Y)\n",
    "    \n",
    "    # Score your validation data and log metrics\n",
    "    score_log_classification_validation_data(xgbModel, X, Y)\n",
    "    print(\"Scoring Done for Validation Data\")\n",
    "\n",
    "    # Log a Confusion Matrix and Precision Recall Curve for your training data\n",
    "    log_classification_charts(\"Validation\", xgbModel, val_X, val_Y)\n",
    "    \n",
    "    # Model Explanations\n",
    "    client = ExplanationClient.from_run(run)\n",
    "    explainer = MimicExplainer(xgbModel, \n",
    "                               X, \n",
    "                               LGBMExplainableModel,\n",
    "                               classes = list(val_Y.unique()),\n",
    "                               features = val_X.columns,\n",
    "                               shap_values_output = 'probability',\n",
    "                               model_task = 'classification')\n",
    "    global_explanation = explainer.explain_global(X)\n",
    "    print(global_explanation)\n",
    "    client.upload_model_explanation(global_explanation, top_k=30)\n",
    "    print(\"Global Explanations Created\")\n",
    "    \n",
    "    # Save local Explanations in json format to a column in the Validation Set\n",
    "    valDF = save_local_explanations(explainer, valDF, val_X)\n",
    "    print(\"Explanations Saved to Validation Data\")\n",
    "    \n",
    "    # Save Global Explanations as a pandas dataframe\n",
    "    globalExplanations = save_global_explanations(explainer, val_X)\n",
    "    print(\"Global Explanations Saved as Pandas Dataframe\")\n",
    "    \n",
    "    # Make a folder in which to save your output\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "    # Save your Model\n",
    "    joblib.dump(xgbModel, 'outputs/XGBmodel.pkl')\n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    # Save your Explainer Model\n",
    "    joblib.dump(explainer, 'outputs/LGBMexplainer.pkl')\n",
    "    print(\"Explainer Model Saved\")\n",
    "    \n",
    "    # Save your Validation Set Predictions\n",
    "    valDF = make_classification_predictions(xgbModel, valDF, val_X, val_Y)\n",
    "    valCSV = valDF.to_csv('outputs/validationPredictions.csv', index=False)\n",
    "    print('Validation Predictions written to CSV file in logs')\n",
    "    \n",
    "    # Save your Global Explanations\n",
    "    globalExplanationsCSV = globalExplanations.to_csv('outputs/globalExplanations.csv', index=False)\n",
    "    print('Global Explanations written to CSV file in logs')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()\n",
    "    print('Script Initialized')\n",
    "    main()\n",
    "    print('Script Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Pipeline Data to pass Best Model to Model Registration Step\n",
    "Pipeline data will be used to pass in combined metrics for all Hyperdrive runs along with the model and explaination for that <b>highest performing model</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Default Datastore\n",
    "defaultDatastore = ws.get_default_datastore()\n",
    "\n",
    "# Hyperdrive Metrics\n",
    "metricsOutputName = 'metrics_output'\n",
    "metricsData = PipelineData(name = 'metrics_data',\n",
    "                           datastore = defaultDatastore,\n",
    "                           pipeline_output_name = metricsOutputName,\n",
    "                           training_output = TrainingOutput(\"Metrics\"))\n",
    "\n",
    "# Hyperdrive Best Model\n",
    "modelOutputName = 'model_output'\n",
    "savedModel = PipelineData(name = 'saved_model',\n",
    "                          datastore = defaultDatastore,\n",
    "                          pipeline_output_name = modelOutputName,\n",
    "                          training_output = TrainingOutput(\"Model\", model_file=\"outputs/XGBmodel.pkl\"))\n",
    "\n",
    "# Hyperdrive Best Model Explanations\n",
    "explainerOutputName = 'explainer_output'\n",
    "explainerModel = PipelineData(name = 'explainer_model',\n",
    "                              datastore = defaultDatastore,\n",
    "                              pipeline_output_name = explainerOutputName,\n",
    "                              training_output = TrainingOutput(\"Model\", model_file=\"outputs/LGBMexplainer.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Model Registration Script\n",
    "This scrip will register your model.  \n",
    "\n",
    "It will only do after comparing your model's performance to older versions if any exist, and also comparing your training results to your validation results.  \n",
    "\n",
    "It will also save out predictions and explanations for your validation data to your datastore <b>using the best model</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./XGB_Pipeline_Scripts/Training/XGB_Hyperdrive_Model_Registration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $modelRegistrationFilePath\n",
    "# Load in libraries\n",
    "import argparse\n",
    "import datetime as dt\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import scipy.stats as st\n",
    "import shutil\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import LGBMExplainableModel\n",
    "from itertools import zip_longest\n",
    "from lightgbm import LGBMClassifier\n",
    "from shutil import copy2\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Load in Azure libraries\n",
    "from azureml.core import Dataset, Datastore, Experiment, Model, Run, Workspace\n",
    "\n",
    "# Load in functions from shared functions file\n",
    "from XGB_Hyperdrive_Shared_Functions import create_dict, set_tags, save_local_explanations, write_to_datastore\n",
    "from XGB_Hyperdrive_Shared_Functions import make_classification_predictions, split_x_y, save_global_explanations\n",
    "\n",
    "# Define script-specific functions\n",
    "def register_model(workspace, modelName, modelPath, trainDataset, valDataset, description, tags):\n",
    "    Model.register(workspace = workspace, model_name = modelName, model_path = modelPath, description = description,\\\n",
    "                   tags = tags, datasets=[('Training', trainDataset),('Validation', valDataset)])\n",
    "    print(\"Registered version {0} of model {1}\".format(model.version, model.name))\n",
    "\n",
    "def load_model_from_hd(modelFolder, savedModel):\n",
    "    copy2(savedModel, modelFolder)\n",
    "    modelPath = modelFolder + 'saved_model'\n",
    "    model = joblib.load(modelPath)\n",
    "    return model\n",
    "\n",
    "def load_explainer_model_from_hd(modelFolder, explainerModel):\n",
    "    copy2(explainerModel, modelFolder)\n",
    "    modelPath = modelFolder + 'explainer_model'\n",
    "    model = joblib.load(modelPath)\n",
    "    return model\n",
    "\n",
    "def load_model(modelName):\n",
    "    modelPath = Model.get_model_path(modelName)\n",
    "    model = joblib.load(modelPath)\n",
    "    return model\n",
    "\n",
    "def score_model(model, features, targetColumn, scoringMethod):\n",
    "    predictions = model.predict(features)\n",
    "    score = scoringMethod(targetColumn, predictions)\n",
    "    return score\n",
    "\n",
    "def set_scoring_method(scoringMethod):\n",
    "    if scoringMethod == 'Accuracy Training':\n",
    "        return accuracy_score\n",
    "    elif scoringMethod == 'Balanced Accuracy Training':\n",
    "        return balanced_accuracy_score\n",
    "    elif scoringMethod == 'Precision Training':\n",
    "        return precision_score\n",
    "    elif scoringMethod == 'Recall Training':\n",
    "        return recall_score\n",
    "    elif scoringMethod == 'F1 Training':\n",
    "        return F1_score\n",
    "    else:\n",
    "        print('Add your scoring metric to the set_scoring_method function')\n",
    "        raise Exception ('Scoring Metric not found in set_scoring_method function.  Add your metric to the function.')\n",
    "\n",
    "def init():\n",
    "    # Set Arguments.\n",
    "    global args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_dataset_name', type=str,\n",
    "                            help='Name of Training Dataset')\n",
    "    parser.add_argument('--val_dataset_name', type=str,\n",
    "                            help='Name of Validation Dataset')\n",
    "    parser.add_argument('--datastore_name', type=str,\n",
    "                            help='Name of Datastore')\n",
    "    parser.add_argument('--project_name', type=str,\n",
    "                            help='Name of project')\n",
    "    parser.add_argument('--project_description', type=str,\n",
    "                            help='Description of project')\n",
    "    parser.add_argument('--pytz_time_zone', type=str,\n",
    "                            help='Time Zone associated with your data')\n",
    "    parser.add_argument('--target_column_name', type=str,\n",
    "                            help='Name of variable to score')\n",
    "    parser.add_argument('--k_folds', type=int, default = 10,\n",
    "                            help='Number of folds to split your data into for cross validation')\n",
    "    parser.add_argument('--confidence_level', type=float, default = 0.95,\n",
    "                            help='Level of confidence to set for your confidence interval ()')\n",
    "    parser.add_argument('--model_name', type=str,\n",
    "                            help='Name of model to register')\n",
    "    parser.add_argument('--output_path', type=str,\n",
    "                            help='Location to store output on Datastore')\n",
    "    parser.add_argument('--scoring_metric', type=str,\n",
    "                            help='Metric with which you scored your Hyperdrive Run')\n",
    "    parser.add_argument('--metric_goal', type=str, default = 'MAXIMIZE',\n",
    "                            help='Whether the scoring metric should be minimized or maximized')\n",
    "    parser.add_argument('--saved_model', type=str, \n",
    "                            help='path to saved model file')\n",
    "    parser.add_argument('--explainer_model', type=str, \n",
    "                            help='path to saved explanation file')\n",
    "    parser.add_argument('--metrics_data', type=str,\n",
    "                            help='Location of Hyperdrive Run Metrics File')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set the Run context for logging\n",
    "    global run\n",
    "    run = Run.get_context()\n",
    "\n",
    "def main():\n",
    "    # Set scoring metric\n",
    "    scoringMethod = set_scoring_method(args.scoring_metric)\n",
    "    print ('Scoring Metric Set')\n",
    "    \n",
    "    # Retrieve your Metrics Data file\n",
    "    with open(args.metrics_data) as metrics:\n",
    "        metricsData = json.load(metrics)\n",
    "    print('Metrics File Downloaded')\n",
    "    \n",
    "    # Turn the Metrics JSON file into a Pandas Dataframe, then transpose and sort it by your scoring metric.\n",
    "    metrics = pd.DataFrame(metricsData)\n",
    "    metricsTransposed = metrics.transpose().sort_values(by=args.scoring_metric, ascending=False)\n",
    "    print('Metrics Dataframe Created')\n",
    "    \n",
    "    # Connect to your AMLS Workspace and set your Datastore\n",
    "    ws = run.experiment.workspace\n",
    "    datastore = Datastore.get(ws, args.datastore_name)\n",
    "    print('Datastore Set')\n",
    "    \n",
    "    # Retrieve your dataset\n",
    "    trainDataset = Dataset.get_by_name(ws, args.train_dataset_name)\n",
    "    valDataset = Dataset.get_by_name(ws, args.val_dataset_name)\n",
    "    print('Datasets Retrieved')\n",
    "    \n",
    "    # Transform your data into Pandas dataframes\n",
    "    trainDF = trainDataset.to_pandas_dataframe()\n",
    "    valDF = valDataset.to_pandas_dataframe()\n",
    "    print('Datasets Converted to Pandas')\n",
    "    \n",
    "    # Split out X and Y variables\n",
    "    val_X, val_Y = split_x_y(valDF, args.target_column_name)\n",
    "    print(\"Validation Data split into Feature and Target Columns\")\n",
    "    \n",
    "    # Load your training model\n",
    "    modelFolder = 'model/'\n",
    "    os.makedirs(modelFolder, exist_ok=True)\n",
    "    newModel = load_model_from_hd(modelFolder, args.saved_model)\n",
    "    print(\"Training Model Loaded\")\n",
    "    \n",
    "    # Load your explainer model\n",
    "    explainer = load_explainer_model_from_hd(modelFolder, args.explainer_model)\n",
    "    print(\"Explainer Model Loaded\")\n",
    "    \n",
    "    # Save explanations to your validation data\n",
    "    valDF = save_local_explanations(explainer, valDF, val_X)\n",
    "    print(\"Explanations Saved to Validation Data\")\n",
    "    \n",
    "    # Save Global Explanations as a pandas dataframe\n",
    "    globalExplanations = save_global_explanations(explainer, val_X)\n",
    "    print(\"Global Explanations Saved as Pandas Dataframe\")\n",
    "    \n",
    "    # Save your Validation Set Predictions\n",
    "    valDF = make_classification_predictions(newModel, valDF, val_X, val_Y)\n",
    "    print('Validation Predictions written to CSV file in logs')\n",
    "    \n",
    "    # Set your Time Zone \n",
    "    timeZone = pytz.timezone(args.pytz_time_zone)\n",
    "    timeLocal = dt.datetime.now(timeZone).strftime('%Y-%m-%d')\n",
    "    print('Time Zone Set')\n",
    "    \n",
    "    # Make Output Directory\n",
    "    datastorePath = args.output_path + '/' + timeLocal\n",
    "    os.makedirs(datastorePath, exist_ok=True) \n",
    "    print('Output Directory Created')\n",
    "    \n",
    "    # Upload Validation Data with Predictions to Datastore\n",
    "    write_to_datastore(valDF, ws, datastore, datastorePath, \"validationPredictions.csv\", False)\n",
    "    print('Predictions with Explanations for Validation Data Loaded to Datastore')\n",
    "    \n",
    "    # Save your Global Explanations\n",
    "    write_to_datastore(globalExplanations, ws, datastore, datastorePath, \"globalExplanations.csv\", False)\n",
    "    print('Global Expanations Loaded to Datastore')\n",
    "    \n",
    "    # Calculate main scoring metric for the validation dataset\n",
    "    newModelScore = score_model(newModel, val_X, val_Y, scoringMethod)\n",
    "    print('Predictions Made for Validation Data')\n",
    "    print('Validation Set ' + args.scoring_metric + ' is ' + str(round(newModelScore, 2)))\n",
    "    \n",
    "    # Retrieve confidence interval for the cross validated scoring metric\n",
    "    lowerBoundColumn = args.scoring_metric + ' Lower CI'\n",
    "    upperBoundColumn = args.scoring_metric + ' Upper CI'\n",
    "    lowerBound = metricsTransposed[lowerBoundColumn][0]\n",
    "    upperBound = metricsTransposed[upperBoundColumn][0]\n",
    "    print('Model ' + str(args.scoring_metric) + ' is ' + str(args.confidence_level*100) +\\\n",
    "          '% likely to actually fall between ' + str(lowerBound) + ' and ' + str(upperBound))\n",
    "    \n",
    "    # Compare confidence interval of cross validation training metric with validation metric\n",
    "    if (((args.metric_goal == 'MAXIMIZE') & (newModelScore < lowerBound)) or\\\n",
    "       (((args.metric_goal == 'MINIMIZE') & (newModelScore > upperBound)))):\n",
    "        print('Model performance on training data is significantly different from performance on validation data.')\n",
    "        raise Exception(\"Models performs differently on training and validation data.\\\n",
    "                         Please check to see if your model is overfitting.  Validation Set \"\\\n",
    "                        + args.scoring_metric + ' is ' + str(round(newModelScore, 2)) + \".  \"\\\n",
    "                        + 'Model ' + str(args.scoring_metric) + ' is ' + str(args.confidence_level*100) +\\\n",
    "                          '% likely to actually fall between ' + str(lowerBound) + ' and ' + str(upperBound))\n",
    "    else:\n",
    "        print('Model is performing as expected on validation data.')\n",
    "    \n",
    "    # Check to see if previous model exists and compare it to new model\n",
    "    modelDictionary = ws.models\n",
    "    if args.model_name in modelDictionary.keys():\n",
    "        print('Models Being Compared')\n",
    "        oldModel = load_model(args.model_name)\n",
    "        oldModelScore = score_model(oldModel, val_X, val_Y, scoringMethod)\n",
    "        print('Previous Model Loaded and is being Compared to New Model')\n",
    "        if newModelScore > oldModelScore:\n",
    "            registerFlag = 1\n",
    "        else:\n",
    "            registerFlag = 0\n",
    "    else:\n",
    "        registerFlag = 1\n",
    "        print('No Previous Models Found')\n",
    "    \n",
    "    # Set your model tags and description\n",
    "    description = args.project_description\n",
    "    explainTags = set_tags(['Algorithm', 'Project', 'Model Type', 'Explainer Type'],\\\n",
    "                            ['XGB', args.project_name, 'Explainer', 'Mimic'])\n",
    "    trainTags = set_tags(['Algorithm', 'Project', 'Model Type'], ['XGB', args.project_name, 'Classification'])\n",
    "    print(\"Model Tags and Description Assigned\")\n",
    "    \n",
    "    # Register your new model and explainer model\n",
    "    if registerFlag == 1:\n",
    "        modelPath = modelFolder + 'saved_model'\n",
    "        register_model(ws, args.model_name, modelPath, trainDataset, valDataset, description, trainTags)\n",
    "        modelNameExplainer = args.model_name + '-Explainer'\n",
    "        explainerPath = modelFolder + 'explainer_model'\n",
    "        register_model(ws, modelNameExplainer, explainerPath, trainDataset, valDataset, description, explainTags)\n",
    "    else:\n",
    "        print('Old model outperforms new model and new model will not be registered.')\n",
    "        \n",
    "    # Remove files from compute cluster\n",
    "    shutil.rmtree(datastorePath)\n",
    "\n",
    "    # Remove model pickle file from compute cluster\n",
    "    shutil.rmtree(modelFolder)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()\n",
    "    print('Script Initialized')\n",
    "    main()\n",
    "    print('Script Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Metrics Output Step\n",
    "This script outputs all of the metrics for of all your Hyperdrive runs into files on your datastore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./XGB_Pipeline_Scripts/Training/XGB_Hyperdrive_Metrics_Output.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $metricsOutputFilePath\n",
    "# Load in libraries\n",
    "import argparse\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import shutil\n",
    "from itertools import zip_longest\n",
    "from shutil import copy2\n",
    "\n",
    "# Load in Azure libraries\n",
    "from azureml.core import Dataset, Datastore, Experiment, Model, Run, Workspace\n",
    "\n",
    "# Load in functions from shared functions file\n",
    "from XGB_Hyperdrive_Shared_Functions import write_to_datastore\n",
    "\n",
    "def init():\n",
    "    # Set Arguments.\n",
    "    global args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--datastore_name', type=str,\n",
    "                            help='Name of Datastore')\n",
    "    parser.add_argument('--pytz_time_zone', type=str,\n",
    "                            help='Time Zone associated with your data')\n",
    "    parser.add_argument('--output_path', type=str,\n",
    "                            help='Location to store output on Datastore')\n",
    "    parser.add_argument('--scoring_metric', type=str,\n",
    "                            help='Metric with which you scored your Hyperdrive Run')\n",
    "    parser.add_argument('--metrics_data', type=str,\n",
    "                            help='Location of Hyperdrive Run Metrics File')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set the Run context for logging\n",
    "    global run\n",
    "    run = Run.get_context()\n",
    "\n",
    "def main():   \n",
    "    # Retrieve your Metrics Data file\n",
    "    with open(args.metrics_data) as metrics:\n",
    "        metricsData = json.load(metrics)\n",
    "    print('Metrics File Downloaded')\n",
    "    \n",
    "    # Turn the Metrics JSON file into two pandas dataframes\n",
    "    metrics = pd.DataFrame(metricsData)\n",
    "    metricsTransposed = metrics.transpose().sort_values(by=args.scoring_metric, ascending=False)\n",
    "    print('Metrics Dataframes Created')\n",
    "    \n",
    "    # Connect to your AMLS Workspace and set your Datastore\n",
    "    ws = run.experiment.workspace\n",
    "    datastore = Datastore.get(ws, args.datastore_name)\n",
    "    print('Datastore Set')\n",
    "    \n",
    "    # Set your Time Zone \n",
    "    timeZone = pytz.timezone(args.pytz_time_zone)\n",
    "    timeLocal = dt.datetime.now(timeZone).strftime('%Y-%m-%d')\n",
    "    print('Time Zone Set')\n",
    "    \n",
    "    # Make Output Directory\n",
    "    outputFolder = args.output_path + '/' + timeLocal\n",
    "    os.makedirs(outputFolder, exist_ok=True) \n",
    "    print('Output Directory Created')\n",
    "    \n",
    "    # Upload csv files to Datastore\n",
    "    write_to_datastore(metrics, ws, datastore, outputFolder, 'Metrics.csv', True)\n",
    "    write_to_datastore(metricsTransposed, ws, datastore, outputFolder, 'MetricsTransposed.csv', True)\n",
    "    print('Hyperdrive Metrics Data Loaded to Datastore')\n",
    "    \n",
    "    # Remove files from compute cluster\n",
    "    shutil.rmtree(outputFolder)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()\n",
    "    print('Script Initialized')\n",
    "    main()\n",
    "    print('Script Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your Pipeline Parameters\n",
    "These are all the parameters you can use to easily adapt this code to other projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Registration Step Parameters\n",
    "train_dataset_name_param = PipelineParameter(name=\"TrainDatasetName\", default_value='None')\n",
    "val_dataset_name_param = PipelineParameter(name=\"ValDatasetName\", default_value='None')\n",
    "datastore_name_param = PipelineParameter(name=\"DatastoreName\", default_value='None')\n",
    "datastore_path_param = PipelineParameter(name=\"DatastorePath\", default_value='None')\n",
    "train_file_name_param = PipelineParameter(name=\"TrainFileName\", default_value='None')\n",
    "val_file_name_param = PipelineParameter(name=\"ValFileName\", default_value='None')\n",
    "project_name_param = PipelineParameter(name=\"ProjectName\", default_value='None')\n",
    "project_description_param = PipelineParameter(name=\"ProjectDescription\", default_value='None')\n",
    "pytz_time_zone_param = PipelineParameter(name='PytzTimeZone', default_value='UTC')\n",
    "\n",
    "# Hyperdrive Step Parameters\n",
    "target_column_param = PipelineParameter(name=\"TargetColumn\", default_value='None')\n",
    "k_folds_param = PipelineParameter(name=\"KFolds\", default_value=10)\n",
    "shuffle_split_size_param = PipelineParameter(name=\"ShuffleSplitSize\", default_value=0.1)\n",
    "confidence_level_param = PipelineParameter(name=\"ConfidenceLevel\", default_value = 0.95)\n",
    "\n",
    "# Model Registration Step Parameters\n",
    "model_name_param = PipelineParameter(name=\"ModelName\", default_value='None')\n",
    "output_path_param = PipelineParameter(name=\"OutputPath\", default_value='None')\n",
    "scoring_metric_param = PipelineParameter(name=\"ScoringMetric\", default_value='None')\n",
    "metric_goal_param = PipelineParameter(name=\"MetricGoal\", default_value='MAXIMIZE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your Unit Testing Step\n",
    "Configure your unit testing step by specifing the folder and file names, the docker container run configuration, and the remote compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitTestingStep = PythonScriptStep(\n",
    "    name = \"unit-testing-step\",\n",
    "    source_directory = projectFolder,\n",
    "    script_name = unitTestingFileName,\n",
    "    arguments=[],\n",
    "    compute_target=computeTarget,\n",
    "    runconfig=runConfig,\n",
    "    allow_reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your Dataset Registration Step\n",
    "Configure your data registration step by specifing the folder and file names, the docker container run configuration, the remote compute target, and parameter arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRegistrationStep = PythonScriptStep(\n",
    "    name = \"dataset-registration-step\",\n",
    "    source_directory = projectFolder,\n",
    "    script_name = datasetRegistrationFileName,\n",
    "    arguments=['--train_dataset_name', train_dataset_name_param,\n",
    "               '--val_dataset_name', val_dataset_name_param,\n",
    "               '--datastore_name', datastore_name_param,\n",
    "               '--datastore_path', datastore_path_param,\n",
    "               '--train_file_name', train_file_name_param,\n",
    "               '--val_file_name', val_file_name_param,\n",
    "               '--project_name', project_name_param,\n",
    "               '--project_description', project_description_param,\n",
    "               '--pytz_time_zone', pytz_time_zone_param],\n",
    "    compute_target=computeTarget,\n",
    "    runconfig=runConfig,\n",
    "    allow_reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your Hyperdrive Step\n",
    "Configure your Hyperdrive registration step by specifing the folder and file names, the run environment, the remote compute target and parameter arguments.  \n",
    "\n",
    "Then, specify which <b>hyperparameters</b> you'd like to tune and the values that should be tested.\n",
    "\n",
    "Next, set the scoring metric and whether that metric should be minimized or maximized, along with the desired number of runs to tune your model.\n",
    "\n",
    "Finally, configure the step to output the best model, the best model explainer, and hyperdrive metrics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your script run configuration\n",
    "scriptRunConfig = ScriptRunConfig(source_directory = projectFolder,\n",
    "                  script = trainingFileName,\n",
    "                  compute_target = computeTarget,\n",
    "                  environment = environment,\n",
    "                  arguments = ['--train_dataset_name', train_dataset_name_param,\n",
    "                               '--val_dataset_name', val_dataset_name_param,\n",
    "                               '--target_column_name', target_column_param,\n",
    "                               '--k_folds', k_folds_param,\n",
    "                               '--shuffle_split_size', shuffle_split_size_param,\n",
    "                               '--confidence_level', confidence_level_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = BayesianParameterSampling({\n",
    "                        '--eta': uniform(0.01, 0.5),\n",
    "                        '--learning_rate': uniform(0.01,0.5),\n",
    "                        '--min_child_weight': uniform(1,100),\n",
    "                        '--max_depth': choice(range(3,11)),\n",
    "                        '--gamma': uniform(0,10),\n",
    "                        '--subsample': uniform(0.5,1),\n",
    "                        '--colsample_bytree': uniform(0.5,1),\n",
    "                        '--reg_lambda': uniform(0,10),\n",
    "                        '--alpha': uniform(0,10),\n",
    "                        '--scale_pos_weight': uniform(0,10),\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For best results with Bayesian Sampling we recommend using a maximum number of runs greater than or equal to 20 times the number of hyperparameters being tuned. Recommendend value:200.\n"
     ]
    }
   ],
   "source": [
    "# Set your Hyperdrive configurations \n",
    "scoringMetric = 'Balanced Accuracy Training'\n",
    "metricGoal = PrimaryMetricGoal.MAXIMIZE\n",
    "metricGoalString = str(metricGoal)[18:]\n",
    "hyperdriveConfig = HyperDriveConfig(run_config = scriptRunConfig,\n",
    "                                     hyperparameter_sampling = hyperParams,\n",
    "                                     primary_metric_name = scoringMetric,\n",
    "                                     primary_metric_goal = metricGoal, # MAXIMIZE OR MINIMIZE\n",
    "                                     max_total_runs = 20,      # should be >= 20 times number of Hyperparameters\n",
    "                                     max_concurrent_runs = 20)  # should be 20 for Bayesian Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your Hyperdrive Step\n",
    "hyperdriveTrainingStep = HyperDriveStep(\n",
    "    name = 'xgb-model-training-step-with-hyperparameter-tuning',\n",
    "    hyperdrive_config = hyperdriveConfig,\n",
    "    inputs = [],\n",
    "    outputs = [metricsData, savedModel, explainerModel],\n",
    "    allow_reuse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your Model Registration Step\n",
    "Configure your model registration step by specifing the folder and file names, the docker container run configuration, the remote compute target, and parameter arguments.\n",
    "\n",
    "Also, take in the best model, best model explanation, and hyperdrive metrics data as input into this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRegistrationStep = PythonScriptStep(\n",
    "    name = \"model-registration-step\",\n",
    "    source_directory = projectFolder,\n",
    "    script_name = modelRegistrationFileName,\n",
    "    inputs = [savedModel, explainerModel, metricsData],\n",
    "    arguments = ['--train_dataset_name', train_dataset_name_param,\n",
    "                 '--val_dataset_name', val_dataset_name_param,\n",
    "                 '--datastore_name', datastore_name_param,\n",
    "                 '--project_name', project_name_param,\n",
    "                 '--project_description', project_description_param,\n",
    "                 '--pytz_time_zone', pytz_time_zone_param,\n",
    "                 '--target_column_name', target_column_param,\n",
    "                 '--k_folds', k_folds_param,\n",
    "                 '--confidence_level', confidence_level_param,\n",
    "                 '--model_name', model_name_param,\n",
    "                 '--output_path', output_path_param,\n",
    "                 '--scoring_metric', scoring_metric_param,\n",
    "                 '--metric_goal', metric_goal_param,\n",
    "                 '--saved_model', savedModel,\n",
    "                 '--explainer_model', explainerModel,\n",
    "                 '--metrics_data', metricsData],\n",
    "    compute_target = computeTarget,\n",
    "    runconfig = runConfig,\n",
    "    allow_reuse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your Hyperdrive Run Metrics Output Step\n",
    "Configure your metrics output step by specifing the folder and file names, the docker container run configuration, the remote compute target, and parameter arguments.\n",
    "\n",
    "Also, take in the hyperdrive metrics data as input into this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsOutputStep = PythonScriptStep(\n",
    "    name = \"metrics-output-step\",\n",
    "    source_directory = projectFolder,\n",
    "    script_name = metricsOutputFileName,\n",
    "    inputs = [metricsData],\n",
    "    arguments = ['--datastore_name', datastore_name_param,\n",
    "                 '--pytz_time_zone', pytz_time_zone_param,\n",
    "                 '--output_path', output_path_param,\n",
    "                 '--scoring_metric', scoring_metric_param,\n",
    "                 '--metrics_data', metricsData],\n",
    "    compute_target = computeTarget,\n",
    "    runconfig = runConfig,\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your Five-Step Pipeline\n",
    "Specify the order in which to run your steps.  Then, pass in your parameters and <b>submit</b> your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your pipeline\n",
    "parallelSteps = [modelRegistrationStep, metricsOutputStep]\n",
    "stepSequence = StepSequence(steps = [unitTestingStep, datasetRegistrationStep, hyperdriveTrainingStep, parallelSteps])\n",
    "pipeline = Pipeline(workspace = ws, steps = stepSequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Explanation\n",
    "<p><b>TrainDatasetName:</b> Name of your registered training dataset.  This can be anything you would like.</p>\n",
    "<p><b>ValDatasetName:</b> Name of your registered validation dataset.  This can be anything you would like.</p>\n",
    "<p><b>DatastoreName:</b> Name of your datastore.  This should be the datastore that holds your input data.</p>\n",
    "<p><b>DatastorePath:</b> Root folder path which holds your data up to today's date.</p>\n",
    "<p><b>TrainFileName:</b> Name of your training file located in your datastore.</p>\n",
    "<p><b>ValFileName:</b> Name of your validation file located in your datastore.</p>\n",
    "<p><b>ProjectName:</b> Name of your project.  This can be anything you would like.</p>\n",
    "<p><b>ProjectDescription:</b> Description of your project.  This can be anything you would like.</p>\n",
    "<p><b>PytzTimeZone:</b> Your timezone or the timezone in which the data is loaded.</p>\n",
    "<p><b>TargetColumn:</b> Name of your target column for machine learning.</p>\n",
    "<p><b>KFolds:</b> Number of times to split your data for cross validation.</p>\n",
    "<p><b>ShuffleSplitSize:</b> Percentage of data to split for cross validation.</p>\n",
    "<p><b>ConfidenceLevel:</b> Percentage used to create your confidence interval to compare validation and training results.</p>\n",
    "<p><b>ModelName:</b> Name of your registered model.  This can anything you like following the naming convention.</p>\n",
    "<p><b>OutputPath:</b> Root folder path to output your results on your datastore.</p>\n",
    "<p><b>ScoringMetric:</b> Metric you wish to maximize or minimize as part of hyperparameter tuning.  Set in the Hyperdrive pipeline step section.</p>\n",
    "<p><b>MetricGoal:</b> Whether you should minimize or maximize your Hyperparameter Metric.  Set in the Hyperdrive pipeline step section.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a list of Pytz Time Zones, uncomment and run the code below\n",
    "#pytz.all_timezones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step unit-testing-step [84fad3e0][be6de64f-a337-4e5c-879b-bfaf3e6d9583], (This step will run and generate new outputs)\n",
      "Created step dataset-registration-step [4dbe6b00][1aea2a40-4db2-4491-bc38-968112cfc047], (This step will run and generate new outputs)\n",
      "Created step xgb-model-training-step-with-hyperparameter-tuning [cc2535e6][fa79cfb8-3a6e-476e-86d4-43232d1c17ba], (This step will run and generate new outputs)\n",
      "Created step model-registration-step [5c64d6b9][1bec9d14-830f-41c0-baf5-0319c8d09cab], (This step will run and generate new outputs)\n",
      "Created step metrics-output-step [0c0ab5be][d24f6134-fd59-422e-bad7-49264a890da3], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 83aec045-2a62-4d8a-bf04-6ee38a921cde\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/83aec045-2a62-4d8a-bf04-6ee38a921cde?wsid=/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourcegroups/ml-teaching/workspaces/ml-teaching-workspace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "source": [
    "# Run your pipeline\n",
    "pipelineName = 'XGB_Model_Training'\n",
    "pipeline_run = Experiment(ws, pipelineName).submit(pipeline,pipeline_parameters=\n",
    "                                                           {'TrainDatasetName': 'XGB Training Data',\n",
    "                                                           'ValDatasetName': 'XGB Validation Data',\n",
    "                                                           'DatastoreName': 'ds_dev',\n",
    "                                                           'DatastorePath': 'XGB/XGB_Training_Input',\n",
    "                                                           'TrainFileName': 'xgbTrainingData.csv',\n",
    "                                                           'ValFileName': 'xgbValidationData.csv',\n",
    "                                                           'ProjectName': 'XGB Test',\n",
    "                                                           'ProjectDescription': 'XGB Test Run',\n",
    "                                                           'PytzTimeZone': 'US/Eastern',\n",
    "                                                           'TargetColumn': 'LeftCompany',\n",
    "                                                           'KFolds': 10,\n",
    "                                                           'ShuffleSplitSize': 0.1,\n",
    "                                                           'ConfidenceLevel': 0.95,\n",
    "                                                           'ModelName': 'Tuned-XGB-Model',\n",
    "                                                           'OutputPath': 'XGB/XGB_Training_Output',\n",
    "                                                           'ScoringMetric': scoringMetric,\n",
    "                                                           'MetricGoal': metricGoalString}, \n",
    "                                                           show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4310df205ec7403991a1b3872262c8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "\"AuthenticationException:\\n\\tMessage: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired\\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"inner_error\\\": {\\n            \\\"code\\\": \\\"Authentication\\\"\\n        },\\n        \\\"message\\\": \\\"Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired\\\"\\n    }\\n}\""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 83aec045-2a62-4d8a-bf04-6ee38a921cde\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/83aec045-2a62-4d8a-bf04-6ee38a921cde?wsid=/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourcegroups/ml-teaching/workspaces/ml-teaching-workspace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df?wsid=/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourcegroups/ml-teaching/workspaces/ml-teaching-workspace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( unit-testing-step ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_631863cdde1aa496170c871ac55bd751278663a436ac21a39c72a53629bacad1_d.txt\n",
      "========================================================================================================================\n",
      "2021-07-22T21:55:26Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21930 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-07-22T21:55:26Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/mounts/workspaceblobstore\n",
      "2021-07-22T21:55:26Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T21:55:26Z Starting output-watcher...\n",
      "2021-07-22T21:55:26Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-07-22T21:55:26Z Executing 'Copy ACR Details file' on 10.0.0.8\n",
      "2021-07-22T21:55:26Z Copy ACR Details file succeeded on 10.0.0.8. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_89938f9de59ccc23393c5d60d6d1f188\n",
      "Digest: sha256:2bfefaa91b273928de538ccf1e5abb5600366a8759874b3e3bad4b95159ae282\n",
      "Status: Image is up to date for mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188:latest\n",
      "mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188:latest\n",
      "2021-07-22T21:55:26Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T21:55:27Z Check if container fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df already exist exited with 0, \n",
      "\n",
      "51aa34f56152da553e8a8a95411414ca6a9ead831568cc3eb93d8c1641a28859\n",
      "2021-07-22T21:55:27Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-07-22T21:55:27Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-de84b2b38056229a5cc4901a5a5637f4-e103e34f9946b586-01 -sshRequired=false] \n",
      "2021/07/22 21:55:27 Starting App Insight Logger for task:  containerSetup\n",
      "2021/07/22 21:55:27 Version: 3.0.01650.0004 Branch: .SourceBranch Commit: 37e4354\n",
      "2021/07/22 21:55:27 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/07/22 21:55:27 Starting infiniband setup\n",
      "2021/07/22 21:55:27 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/07/22 21:55:27 Returning Python Version as 3.6\n",
      "2021-07-22T21:55:27Z VMSize: standard_ds3_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/07/22 21:55:27 VMSize: standard_ds3_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/07/22 21:55:27 VMSize: standard_ds3_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/07/22 21:55:27 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021-07-22T21:55:27Z Not setting up Infiniband in Container\n",
      "2021/07/22 21:55:27 Not setting up Infiniband in Container\n",
      "2021/07/22 21:55:27 Not setting up Infiniband in Container\n",
      "2021/07/22 21:55:27 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/07/22 21:55:27 Returning Python Version as 3.6\n",
      "2021/07/22 21:55:27 sshd inside container not required for job, skipping setup.\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_631863cdde1aa496170c871ac55bd751278663a436ac21a39c72a53629bacad1_d.txt\n",
      "===============================================================================================================\n",
      "[2021-07-22T21:55:37.007784] Entering job release\n",
      "[2021-07-22T21:55:37.884832] Starting job release\n",
      "[2021-07-22T21:55:37.885359] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 120\n",
      "[2021-07-22T21:55:37.885810] job release stage : upload_datastore starting...\n",
      "[2021-07-22T21:55:37.888175] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-07-22T21:55:37.888214] job release stage : execute_job_release starting...\n",
      "[2021-07-22T21:55:37.894563] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-07-22T21:55:37.894615] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-07-22T21:55:37.895304] Entering context manager injector.\n",
      "[2021-07-22T21:55:37.897274] job release stage : upload_datastore completed...\n",
      "[2021-07-22T21:55:37.969475] job release stage : send_run_telemetry starting...\n",
      "[2021-07-22T21:55:37.983536] get vm size and vm region successfully.\n",
      "[2021-07-22T21:55:37.989327] get compute meta data successfully.\n",
      "Failed to upload compute record artifact, error_details=module 'azureml_globals' has no attribute 'compute_rcord_artifact_path'\n",
      "[2021-07-22T21:55:37.989837] upload compute record artifact successfully.\n",
      "[2021-07-22T21:55:37.989924] job release stage : send_run_telemetry completed...\n",
      "[2021-07-22T21:55:38.085925] job release stage : execute_job_release completed...\n",
      "[2021-07-22T21:55:38.086159] Job release is complete\n",
      "\n",
      "StepRun(unit-testing-step) Execution Summary\n",
      "=============================================\n",
      "StepRun( unit-testing-step ) Status: Finished\n",
      "{'runId': 'fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df', 'target': 'automl-cluster', 'status': 'Completed', 'startTimeUtc': '2021-07-22T21:55:26.07261Z', 'endTimeUtc': '2021-07-22T21:55:47.378648Z', 'properties': {'ContentSnapshotId': '262efdb6-ea36-48ee-ab1a-2df4fd2ba574', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'be6de64f-a337-4e5c-879b-bfaf3e6d9583', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '84fad3e0', 'azureml.pipelinerunid': '83aec045-2a62-4d8a-bf04-6ee38a921cde', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'azureml.RuntimeType': 'Hosttools'}, 'inputDatasets': [], 'outputDatasets': [], 'runDefinition': {'script': 'XGB_Hyperdrive_Unit_Testing.py', 'command': '', 'useAbsolutePath': False, 'arguments': [], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'automl-cluster', 'dataReferences': {}, 'data': {}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'XGBoostTrainingEnv', 'version': '1', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults==1.31.0', 'azureml-interpret==1.31.0', 'azureml-explain-model==1.31.0', 'pyarrow==1.0.1', 'pytz==2021.1', 'interpret-core==0.1.21', 'lightgbm==2.3.0']}, 'scikit-learn==0.22.1', 'numpy==1.16.2', 'matplotlib==3.2.1', 'joblib==0.14.1', 'xgboost==0.90', 'seaborn==0.9.0', 'pandas==0.23.4', 'scipy==1.3.1'], 'name': 'azureml_d6837a920ac9d0a042e4b8fb48440fae'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210531.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'enableAzmlInt': True, 'priority': None, 'slaTier': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_631863cdde1aa496170c871ac55bd751278663a436ac21a39c72a53629bacad1_d.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/azureml-logs/55_azureml-execution-tvmps_631863cdde1aa496170c871ac55bd751278663a436ac21a39c72a53629bacad1_d.txt?sv=2019-02-02&sr=b&sig=znJRa1Kn6XgKprYaRmEhU4dOCHxiffnZZydRIzdvORI%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'azureml-logs/65_job_prep-tvmps_631863cdde1aa496170c871ac55bd751278663a436ac21a39c72a53629bacad1_d.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/azureml-logs/65_job_prep-tvmps_631863cdde1aa496170c871ac55bd751278663a436ac21a39c72a53629bacad1_d.txt?sv=2019-02-02&sr=b&sig=5ywEQStGEhIxILICWq4bDapjltuHVRtdSIN55gH%2B5Ic%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=dCg1SqsFn002MS%2FDy%2FKXXdwrTulP8dA0IjUczk0mpE0%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'azureml-logs/75_job_post-tvmps_631863cdde1aa496170c871ac55bd751278663a436ac21a39c72a53629bacad1_d.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/azureml-logs/75_job_post-tvmps_631863cdde1aa496170c871ac55bd751278663a436ac21a39c72a53629bacad1_d.txt?sv=2019-02-02&sr=b&sig=Wf%2Byy3Wf8mue6EWOctLkIn9wRi9kmQ%2BTuz%2Bk8%2BYtmr8%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'azureml-logs/process_info.json': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=cubZ6bYkcSMPNArWgbE4pRdXWoGFlcgrNscY1u7BJ84%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'azureml-logs/process_status.json': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=wHH%2B5cdPtz67GWbEn8K90IEe52Rttvat2OfBkXHArcg%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'logs/azureml/94_azureml.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/logs/azureml/94_azureml.log?sv=2019-02-02&sr=b&sig=wfPU7EmCBGEJ7r6R0h89X7myIirD8Va1FSJQI1P22r4%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=Jc75s%2Bs%2Bgql60J2zgjHfi9sU2530fAZMxOG%2FEOJg4TQ%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=uiOXBLUCrLfbtnqoexfnhqq6D5dDR%2FFqkkIPw0nXUVU%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=cwWzRt%2FTy3jfpVLTn0aglMBKlX72caiKgle6rMHJcj0%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=FmDVu7VaUuB6ArxMV1iVhdF6tJpR3uh32qlRqkcnyAk%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.fd3eb2bb-e2a5-4897-ab7a-811a3dc4b5df/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=syeWGw64Czcq4BtV4cE0faAdAucML3PXiGbDQHw9vQI%3D&st=2021-07-22T21%3A45%3A40Z&se=2021-07-23T05%3A55%3A40Z&sp=r'}, 'submittedBy': 'Dennis Sawyers'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StepRunId: a0641745-601b-4995-83c8-a507c264b253\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/a0641745-601b-4995-83c8-a507c264b253?wsid=/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourcegroups/ml-teaching/workspaces/ml-teaching-workspace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( dataset-registration-step ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt\n",
      "========================================================================================================================\n",
      "2021-07-22T21:55:58Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21930 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-07-22T21:55:58Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore\n",
      "2021-07-22T21:55:58Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T21:55:58Z Starting output-watcher...\n",
      "2021-07-22T21:55:58Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-07-22T21:55:58Z Executing 'Copy ACR Details file' on 10.0.0.10\n",
      "2021-07-22T21:55:58Z Copy ACR Details file succeeded on 10.0.0.10. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_89938f9de59ccc23393c5d60d6d1f188\n",
      "Digest: sha256:2bfefaa91b273928de538ccf1e5abb5600366a8759874b3e3bad4b95159ae282\n",
      "Status: Image is up to date for mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188:latest\n",
      "mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188:latest\n",
      "2021-07-22T21:55:59Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T21:55:59Z Check if container a0641745-601b-4995-83c8-a507c264b253 already exist exited with 0, \n",
      "\n",
      "4a677482b77c651bec031c1dcf16106bbe3abe9af9d725506cfadac89c7f53db\n",
      "2021-07-22T21:55:59Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-07-22T21:55:59Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-fda1ead0f9722314dcda09a8f5591bfc-172da7cc2418abbd-01 -sshRequired=false] \n",
      "2021/07/22 21:55:59 Starting App Insight Logger for task:  containerSetup\n",
      "2021/07/22 21:55:59 Version: 3.0.01650.0004 Branch: .SourceBranch Commit: 37e4354\n",
      "2021/07/22 21:55:59 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/07/22 21:55:59 Starting infiniband setup\n",
      "2021/07/22 21:55:59 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/07/22 21:55:59 Returning Python Version as 3.6\n",
      "2021/07/22 21:55:59 VMSize: standard_ds3_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/07/22 21:55:59 VMSize: standard_ds3_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021-07-22T21:55:59Z VMSize: standard_ds3_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/07/22 21:55:59 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/07/22 21:55:59 Not setting up Infiniband in Container\n",
      "2021/07/22 21:55:59 Not setting up Infiniband in Container\n",
      "2021-07-22T21:55:59Z Not setting up Infiniband in Container\n",
      "2021/07/22 21:55:59 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/07/22 21:55:59 Returning Python Version as 3.6\n",
      "2021/07/22 21:55:59 sshd inside container not required for job, skipping setup.\n",
      "2021/07/22 21:56:00 All App Insights Logs was sent successfully or the close timeout of 20 was reached\n",
      "2021/07/22 21:56:00 App Insight Client has already been closed\n",
      "2021/07/22 21:56:00 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-07-22T21:56:00Z Starting docker container succeeded.\n",
      "2021-07-22T21:56:03Z Job environment preparation succeeded on 10.0.0.10. Output: \n",
      ">>>   2021/07/22 21:55:58 Starting App Insight Logger for task:  prepareJobEnvironment\n",
      ">>>   2021/07/22 21:55:58 Version: 3.0.01650.0004 Branch: .SourceBranch Commit: 37e4354\n",
      ">>>   2021/07/22 21:55:58 runtime.GOOS linux\n",
      ">>>   2021/07/22 21:55:58 Checking if '/tmp' exists\n",
      ">>>   2021/07/22 21:55:58 Reading dyanamic configs\n",
      ">>>   2021/07/22 21:55:58 Container sas url: https://baiscriptsdm1prod.blob.core.windows.net/aihosttools?sv=2018-03-28&sr=c&si=aihosttoolspolicy&sig=0dYPjOcglC72KKRE2ILzFCKnPug7ECc2SIRWA3udLW0%3D\n",
      ">>>   2021/07/22 21:55:58 Failed to read from file /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables, open /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables: no such file or directory\n",
      ">>>   2021/07/22 21:55:58 [in autoUpgradeFromJobNodeSetup] Is Azsecpack installer on host: false. Is Azsecpack installation enabled: false,\n",
      ">>>   2021/07/22 21:55:58 Starting Azsecpack installation on machine: 7283217cffb4431c9c10e4642853485200000F#72f988bf-86f1-41af-91ab-2d7cd011db47#47a7ec0c-37ad-428b-9114-b87ea1057632#ml-teaching#ml-teaching-workspace#automl-cluster#tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d\n",
      ">>>   2021/07/22 21:55:58 Is Azsecpack enabled: false, GetDisableVsatlsscan: true\n",
      ">>>   2021/07/22 21:55:58 Turning off azsecpack, if it is already running\n",
      ">>>   2021/07/22 21:55:58 [doTurnOffAzsecpack] output:Unit mdsd.service could not be found.\n",
      ">>>   ,err:exit status 1.\n",
      ">>>   2021/07/22 21:55:58 OS patching disabled by dynamic configs. Skipping.\n",
      ">>>   2021/07/22 21:55:58 DetonationChamber is not enabled on this subscription: 47a7ec0c-37ad-428b-9114-b87ea1057632\n",
      ">>>   2021/07/22 21:55:58 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 21:55:58 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 21:55:58 Get GPU count failed with err: The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command., \n",
      ">>>   2021/07/22 21:55:58 AMLComputeXDSEndpoint:  https://dm1-prodk8ds.batchai.core.windows.net\n",
      ">>>   2021/07/22 21:55:58 AMLComputeXDSApiVersion:  2018-02-01\n",
      ">>>   2021/07/22 21:55:58 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/config\n",
      ">>>   2021/07/22 21:55:58 This is not a aml-workstation (compute instance), current offer type: amlcompute. Starting identity responder as part of prepareJobEnvironment.\n",
      ">>>   2021/07/22 21:55:58 Starting identity responder.\n",
      ">>>   2021/07/22 21:55:58 Starting identity responder.\n",
      ">>>   2021/07/22 21:55:58 Failed to open file /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/config/.batchai.IdentityResponder.envlist: open /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/config/.batchai.IdentityResponder.envlist: no such file or directory\n",
      ">>>   2021/07/22 21:55:58 Logfile used for identity responder: /mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/IdentityResponderLog-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt\n",
      ">>>   2021/07/22 21:55:58 Logfile used for identity responder: /mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/IdentityResponderLog-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt\n",
      ">>>   2021/07/22 21:55:58 Started Identity Responder for job.\n",
      ">>>   2021/07/22 21:55:58 Started Identity Responder for job.\n",
      ">>>   2021/07/22 21:55:58 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd\n",
      ">>>   2021/07/22 21:55:58 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/shared\n",
      ">>>   2021/07/22 21:55:58 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/07/22 21:55:58 Mounting job level file systems\n",
      ">>>   2021/07/22 21:55:58 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts\n",
      ">>>   2021/07/22 21:55:58 Attempting to read datastore credentials file: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/config/.amlcompute.datastorecredentials\n",
      ">>>   2021/07/22 21:55:58 Datastore credentials file not found, skipping.\n",
      ">>>   2021/07/22 21:55:58 Attempting to read runtime sas tokens file: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/config/.master.runtimesastokens\n",
      ">>>   2021/07/22 21:55:58 Runtime sas tokens file not found, skipping.\n",
      ">>>   2021/07/22 21:55:58 No NFS configured\n",
      ">>>   2021/07/22 21:55:58 No Azure File Shares configured\n",
      ">>>   2021/07/22 21:55:58 Mounting blob file systems\n",
      ">>>   2021/07/22 21:55:58 Blobfuse runtime version 1.3.6\n",
      ">>>   2021/07/22 21:55:58 Mounting azureml-blobstore-d8c754f2-1214-460a-b696-a73d7a887cd1 container from mlteachingwork0962888719 account at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore\n",
      ">>>   2021/07/22 21:55:58 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/07/22 21:55:58 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/07/22 21:55:58 Blobfuse cache size set to 21930 MB.\n",
      ">>>   2021/07/22 21:55:58 Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21930 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      ">>>   2021/07/22 21:55:58 Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore\n",
      ">>>   2021/07/22 21:55:58 Waiting for blobfs to be mounted at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore\n",
      ">>>   2021/07/22 21:55:58 Successfully mounted azureml-blobstore-d8c754f2-1214-460a-b696-a73d7a887cd1 container from mlteachingwork0962888719 account at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore\n",
      ">>>   2021/07/22 21:55:58 Failed to created run_id directory: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore/azureml/a0641745-601b-4995-83c8-a507c264b253, due to mkdir /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore/azureml/a0641745-601b-4995-83c8-a507c264b253: read-only file system\n",
      ">>>   2021/07/22 21:55:58 No unmanaged file systems configured\n",
      ">>>   2021/07/22 21:55:58 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 21:55:58 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 21:55:58 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/07/22 21:55:58 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/azureml_compute_logs\n",
      ">>>   2021/07/22 21:55:58 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/logs\n",
      ">>>   2021/07/22 21:55:58 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/outputs\n",
      ">>>   2021/07/22 21:55:58 Starting output-watcher...\n",
      ">>>   2021/07/22 21:55:58 Single file input dataset is enabled.\n",
      ">>>   2021/07/22 21:55:58 Start to pulling docker image: mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188\n",
      ">>>   2021/07/22 21:55:58 Start pull docker image: mlteachingwoe1b28e33.azurecr.io\n",
      ">>>   2021/07/22 21:55:58 Getting credentials for image mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188 with url mlteachingwoe1b28e33.azurecr.io\n",
      ">>>   2021/07/22 21:55:58 Container registry is ACR.\n",
      ">>>   2021/07/22 21:55:58 Skip getting ACR Credentials from Identity and will be getting it from EMS\n",
      ">>>   2021/07/22 21:55:58 Getting ACR Credentials from EMS for environment XGBoostTrainingEnv:Autosave_2021-07-20T05:54:36Z_5722c198\n",
      ">>>   2021/07/22 21:55:58 Requesting XDS for registry details.\n",
      ">>>   2021/07/22 21:55:58 Attempt 1 of http call to https://dm1-prodk8ds.batchai.core.windows.net/hosttoolapi/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourceGroups/ml-teaching/workspaces/ml-teaching-workspace/clusters/automl-cluster/nodes/tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d?api-version=2018-02-01\n",
      ">>>   2021/07/22 21:55:58 Got container registry details from credentials service for registry address: mlteachingwoe1b28e33.azurecr.io.\n",
      ">>>   2021/07/22 21:55:58 Writing ACR Details to file...\n",
      ">>>   2021/07/22 21:55:58 Copying ACR Details file to worker nodes...\n",
      ">>>   2021/07/22 21:55:58 Executing 'Copy ACR Details file' on 10.0.0.10\n",
      ">>>   2021/07/22 21:55:58 Begin executing 'Copy ACR Details file' task on Node\n",
      ">>>   2021/07/22 21:55:58 'Copy ACR Details file' task Node result: succeeded\n",
      ">>>   2021/07/22 21:55:58 Copy ACR Details file succeeded on 10.0.0.10. Output: \n",
      ">>>   >>>   \n",
      ">>>   >>>   \n",
      ">>>   2021/07/22 21:55:58 Successfully retrieved ACR Credentials from EMS.\n",
      ">>>   2021/07/22 21:55:58 EMS returned mlteachingwoe1b28e33.azurecr.io for environment XGBoostTrainingEnv\n",
      ">>>   2021/07/22 21:55:58 Save docker credentials for image mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188 in /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/docker_login_DEF432A09A7B137F\n",
      ">>>   2021/07/22 21:55:58 Start login to the docker registry\n",
      ">>>   2021/07/22 21:55:59 Successfully logged into the docker registry.\n",
      ">>>   2021/07/22 21:55:59 Start run pull docker image command\n",
      ">>>   2021/07/22 21:55:59 Pull docker image succeeded.\n",
      ">>>   2021/07/22 21:55:59 Removed docker config dir /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/docker_login_DEF432A09A7B137F\n",
      ">>>   2021/07/22 21:55:59 Pull docker image time: 298.268068ms\n",
      ">>>   \n",
      ">>>   2021/07/22 21:55:59 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/07/22 21:55:59 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 21:55:59 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 21:55:59 Setting the memory limit for docker container to be 13674 MB\n",
      ">>>   2021/07/22 21:55:59 The env variable file size is 40202 bytes\n",
      ">>>   2021/07/22 21:55:59 Creating parent cgroup 'a0641745-601b-4995-83c8-a507c264b253' for Containers used in Job\n",
      ">>>   2021/07/22 21:55:59 Add parent cgroup 'a0641745-601b-4995-83c8-a507c264b253' to container 'a0641745-601b-4995-83c8-a507c264b253'\n",
      ">>>   2021/07/22 21:55:59 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      ">>>   2021/07/22 21:55:59 Original Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,a0641745-601b-4995-83c8-a507c264b253,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/certs:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/certs,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-m,13674m,-v,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/azureml_compute_logs,-v,/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/wd:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/wd,-v,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253,-v,/mnt/batch/tasks/shared/LS_root/shared/tracing/a0641745-601b-4995-83c8-a507c264b253/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/a0641745-601b-4995-83c8-a507c264b253/logs/azureml/tracing,-w,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/config/.batchai.envlist,--cgroup-parent=/a0641745-601b-4995-83c8-a507c264b253/,--shm-size,2g\n",
      ">>>   2021/07/22 21:55:59 the binding /mnt/batch/tasks/shared/LS_root/shared/tracing/a0641745-601b-4995-83c8-a507c264b253/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/a0641745-601b-4995-83c8-a507c264b253/logs/azureml/tracing is discarded as we already have /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared \n",
      ">>>   2021/07/22 21:55:59 the binding /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/azureml_compute_logs is discarded as we already have /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253 \n",
      ">>>   2021/07/22 21:55:59 Updated Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,a0641745-601b-4995-83c8-a507c264b253,-m,13674m,-w,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/config/.batchai.envlist,--cgroup-parent=/a0641745-601b-4995-83c8-a507c264b253/,--shm-size,2g,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253,-v,/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/wd:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/wd,-v,/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/certs:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/certs\n",
      ">>>   2021/07/22 21:55:59 Running Docker command: docker run --ulimit memlock=9223372036854775807 --ulimit nofile=262144:262144 --cap-add sys_ptrace --name a0641745-601b-4995-83c8-a507c264b253 -m 13674m -w /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd --expose 23 --env-file /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/config/.batchai.envlist --cgroup-parent=/a0641745-601b-4995-83c8-a507c264b253/ --shm-size 2g -v /mnt/batch/tasks/startup:/mnt/batch/tasks/startup -v /mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared -v /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared -v /mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs -v /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253 -v /mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/wd:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/wd -v /mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/certs:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/certs -d -it --privileged --net=host mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188\n",
      ">>>   2021/07/22 21:55:59 Check if container a0641745-601b-4995-83c8-a507c264b253 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/07/22 21:55:59 Check if container a0641745-601b-4995-83c8-a507c264b253 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/07/22 21:55:59 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/07/22 21:55:59 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/07/22 21:55:59 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-fda1ead0f9722314dcda09a8f5591bfc-172da7cc2418abbd-01 -sshRequired=false] \n",
      ">>>   2021/07/22 21:55:59 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-fda1ead0f9722314dcda09a8f5591bfc-172da7cc2418abbd-01 -sshRequired=false] \n",
      ">>>   2021/07/22 21:56:00 Container ssh is not required for job type.\n",
      ">>>   2021/07/22 21:56:00 Starting docker container succeeded.\n",
      ">>>   2021/07/22 21:56:00 Starting docker container succeeded.\n",
      ">>>   2021/07/22 21:56:00 Disk space after starting docker container: 23383MB\n",
      ">>>   2021/07/22 21:56:00 Begin execution of runSpecialJobTask\n",
      ">>>   2021/07/22 21:56:00 runSpecialJobTask: os.GetEnv constants.StdouterrDir: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/azureml_compute_logs\n",
      ">>>   2021/07/22 21:56:00 runSpecialJobTask: Raw cmd for preparation is passed is: /azureml-envs/azureml_d6837a920ac9d0a042e4b8fb48440fae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore/azureml/a0641745-601b-4995-83c8-a507c264b253-setup/job_prep.py --snapshots '[{\"Id\":\"262efdb6-ea36-48ee-ab1a-2df4fd2ba574\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/07/22 21:56:00 runSpecialJobTask: stdout path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/azureml_compute_logs/65_job_prep-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt\n",
      ">>>   2021/07/22 21:56:00 runSpecialJobTask: stderr path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253/azureml_compute_logs/65_job_prep-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt\n",
      ">>>   2021/07/22 21:56:00 native cmd: export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253;/azureml-envs/azureml_d6837a920ac9d0a042e4b8fb48440fae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore/azureml/a0641745-601b-4995-83c8-a507c264b253-setup/job_prep.py --snapshots '[{\"Id\":\"262efdb6-ea36-48ee-ab1a-2df4fd2ba574\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/07/22 21:56:00 runSpecialJobTask: commons.GetOsPlatform(): ubuntu\n",
      ">>>   2021/07/22 21:56:00 runSpecialJobTask: Running cmd: /usr/bin/docker exec -e AZUREML_SDK_TRACEPARENT=00-fda1ead0f9722314dcda09a8f5591bfc-d83aaca4546a946d-01 -t a0641745-601b-4995-83c8-a507c264b253 bash -c if [ -f ~/.bashrc ]; then PS1_back=$PS1; PS1='$'; . ~/.bashrc; PS1=$PS1_back; fi;PATH=$PATH:$AZ_BATCH_NODE_STARTUP_DIR/wd/;export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/a0641745-601b-4995-8_b45917a3-044d-42d2-b214-f913236668b5/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/wd/azureml/a0641745-601b-4995-83c8-a507c264b253;/azureml-envs/azureml_d6837a920ac9d0a042e4b8fb48440fae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/a0641745-601b-4995-83c8-a507c264b253/mounts/workspaceblobstore/azureml/a0641745-601b-4995-83c8-a507c264b253-setup/job_prep.py --snapshots '[{\"Id\":\"262efdb6-ea36-48ee-ab1a-2df4fd2ba574\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: job preparation exited with code 0 and err <nil>\n",
      ">>>   \n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:00.631685] Entering job preparation.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:01.894478] Starting job preparation.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:01.894521] Extracting the control code.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:01.894905] Starting extract_project.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:01.894952] Starting to extract zip file.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:01.917466] Finished extracting zip file.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:01.920697] Using urllib.request Python 3.0 or later\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:01.920740] Start fetching snapshots.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:01.920777] Start fetching snapshot.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:01.920796] Retrieving project from snapshot: 262efdb6-ea36-48ee-ab1a-2df4fd2ba574\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: Starting the daemon thread to refresh tokens in background for process with pid = 44\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:02.492749] Finished fetching snapshot.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:02.492779] Finished fetching snapshots.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:02.492799] Finished extract_project.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:02.492879] Finished fetching and extracting the control code.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:02.496207] downloadDataStore - Download from datastores if requested.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:02.496965] Start run_history_prep.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:02.510041] Entering context manager injector.\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:02.514052] downloadDataStore completed\n",
      ">>>   2021/07/22 21:56:02 runSpecialJobTask: preparation: [2021-07-22T21:56:02.515435] Job preparation is complete.\n",
      ">>>   2021/07/22 21:56:02 Execution of runSpecialJobTask completed\n",
      ">>>   2021/07/22 21:56:02 Attempt 1 of http call to https://centralus.api.azureml.ms/history/v1.0/private/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourceGroups/ml-teaching/providers/Microsoft.MachineLearningServices/workspaces/ml-teaching-workspace/runs/a0641745-601b-4995-83c8-a507c264b253/spans\n",
      ">>>   2021/07/22 21:56:02 Process Exiting with Code:  0\n",
      ">>>   2021/07/22 21:56:03 All App Insights Logs was sent successfully or the close timeout of 20 was reached\n",
      ">>>   \n",
      "2021-07-22T21:56:03Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T21:56:03Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T21:56:03Z 127.0.0.1 slots=4 max-slots=4\n",
      "2021-07-22T21:56:03Z launching Custom job\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt\n",
      "===============================================================================================================\n",
      "[2021-07-22T21:56:17.894414] Entering job release\n",
      "[2021-07-22T21:56:18.798671] Starting job release\n",
      "[2021-07-22T21:56:18.799217] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 325\n",
      "[2021-07-22T21:56:18.801757] job release stage : upload_datastore starting...\n",
      "[2021-07-22T21:56:18.801933] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-07-22T21:56:18.808076] job release stage : execute_job_release starting...\n",
      "[2021-07-22T21:56:18.808476] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-07-22T21:56:18.808521] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-07-22T21:56:18.810066] Entering context manager injector.\n",
      "[2021-07-22T21:56:18.813050] job release stage : upload_datastore completed...\n",
      "[2021-07-22T21:56:18.889714] job release stage : send_run_telemetry starting...\n",
      "[2021-07-22T21:56:18.903096] get vm size and vm region successfully.\n",
      "[2021-07-22T21:56:18.908428] get compute meta data successfully.\n",
      "Failed to upload compute record artifact, error_details=module 'azureml_globals' has no attribute 'compute_rcord_artifact_path'\n",
      "[2021-07-22T21:56:18.908925] upload compute record artifact successfully.\n",
      "[2021-07-22T21:56:18.909011] job release stage : send_run_telemetry completed...\n",
      "[2021-07-22T21:56:19.006617] job release stage : execute_job_release completed...\n",
      "[2021-07-22T21:56:19.006885] Job release is complete\n",
      "\n",
      "StepRun(dataset-registration-step) Execution Summary\n",
      "=====================================================\n",
      "StepRun( dataset-registration-step ) Status: Finished\n",
      "{'runId': 'a0641745-601b-4995-83c8-a507c264b253', 'target': 'automl-cluster', 'status': 'Completed', 'startTimeUtc': '2021-07-22T21:55:57.494274Z', 'endTimeUtc': '2021-07-22T21:56:28.306693Z', 'properties': {'ContentSnapshotId': '262efdb6-ea36-48ee-ab1a-2df4fd2ba574', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '1aea2a40-4db2-4491-bc38-968112cfc047', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '4dbe6b00', 'azureml.pipelinerunid': '83aec045-2a62-4d8a-bf04-6ee38a921cde', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'azureml.RuntimeType': 'Hosttools'}, 'inputDatasets': [], 'outputDatasets': [{'identifier': {'savedId': '235da23e-b7b7-4c51-9dbd-7f53a55e3542', 'registeredId': '027a2fe0-3d54-4b9a-a61c-2328db9ecc92', 'registeredVersion': '10'}, 'outputType': 'Reference', 'dataset': {\n",
      "  \"source\": [\n",
      "    \"('ds_dev', 'XGB/XGB_Training_Input/2021-07-22/xgbTrainingData.csv')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\",\n",
      "    \"ParseDelimited\",\n",
      "    \"DropColumns\",\n",
      "    \"SetColumnTypes\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"235da23e-b7b7-4c51-9dbd-7f53a55e3542\",\n",
      "    \"name\": \"XGB Training Data\",\n",
      "    \"version\": 10,\n",
      "    \"description\": \"XGB Test Run\",\n",
      "    \"tags\": {\n",
      "      \"Project\": \"XGB Test\",\n",
      "      \"Dataset Type\": \"Training\",\n",
      "      \"Date Created\": \"2021-07-22\"\n",
      "    },\n",
      "    \"workspace\": \"Workspace.create(name='ml-teaching-workspace', subscription_id='47a7ec0c-37ad-428b-9114-b87ea1057632', resource_group='ml-teaching')\"\n",
      "  }\n",
      "}}, {'identifier': {'savedId': '73a3da00-3023-4c53-9599-2b50c6726b93', 'registeredId': 'a6226c8d-e9c0-4463-93d9-47753623ed71', 'registeredVersion': '11'}, 'outputType': 'Reference', 'dataset': {\n",
      "  \"source\": [\n",
      "    \"('ds_dev', 'XGB/XGB_Training_Input/2021-07-22/xgbValidationData.csv')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\",\n",
      "    \"ParseDelimited\",\n",
      "    \"DropColumns\",\n",
      "    \"SetColumnTypes\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"73a3da00-3023-4c53-9599-2b50c6726b93\",\n",
      "    \"name\": \"XGB Validation Data\",\n",
      "    \"version\": 11,\n",
      "    \"description\": \"XGB Test Run\",\n",
      "    \"tags\": {\n",
      "      \"Project\": \"XGB Test\",\n",
      "      \"Dataset Type\": \"Validation\",\n",
      "      \"Date Created\": \"2021-07-22\"\n",
      "    },\n",
      "    \"workspace\": \"Workspace.create(name='ml-teaching-workspace', subscription_id='47a7ec0c-37ad-428b-9114-b87ea1057632', resource_group='ml-teaching')\"\n",
      "  }\n",
      "}}], 'runDefinition': {'script': 'XGB_Hyperdrive_Dataset_Registration.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--train_dataset_name', '$AML_PARAMETER_TrainDatasetName', '--val_dataset_name', '$AML_PARAMETER_ValDatasetName', '--datastore_name', '$AML_PARAMETER_DatastoreName', '--datastore_path', '$AML_PARAMETER_DatastorePath', '--train_file_name', '$AML_PARAMETER_TrainFileName', '--val_file_name', '$AML_PARAMETER_ValFileName', '--project_name', '$AML_PARAMETER_ProjectName', '--project_description', '$AML_PARAMETER_ProjectDescription', '--pytz_time_zone', '$AML_PARAMETER_PytzTimeZone'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'automl-cluster', 'dataReferences': {}, 'data': {}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'XGBoostTrainingEnv', 'version': 'Autosave_2021-07-20T05:54:36Z_5722c198', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults==1.31.0', 'azureml-interpret==1.31.0', 'azureml-explain-model==1.31.0', 'pyarrow==1.0.1', 'pytz==2021.1', 'interpret-core==0.1.21', 'lightgbm==2.3.0']}, 'scikit-learn==0.22.1', 'numpy==1.16.2', 'matplotlib==3.2.1', 'joblib==0.14.1', 'xgboost==0.90', 'seaborn==0.9.0', 'pandas==0.23.4', 'scipy==1.3.1'], 'name': 'azureml_d6837a920ac9d0a042e4b8fb48440fae'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AML_PARAMETER_TrainDatasetName': 'XGB Training Data', 'AML_PARAMETER_ValDatasetName': 'XGB Validation Data', 'AML_PARAMETER_DatastoreName': 'ds_dev', 'AML_PARAMETER_DatastorePath': 'XGB/XGB_Training_Input', 'AML_PARAMETER_TrainFileName': 'xgbTrainingData.csv', 'AML_PARAMETER_ValFileName': 'xgbValidationData.csv', 'AML_PARAMETER_ProjectName': 'XGB Test', 'AML_PARAMETER_ProjectDescription': 'XGB Test Run', 'AML_PARAMETER_PytzTimeZone': 'US/Eastern'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210531.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'enableAzmlInt': True, 'priority': None, 'slaTier': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/azureml-logs/55_azureml-execution-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt?sv=2019-02-02&sr=b&sig=PdhvmaAXHjinRBtvSW88BQOFzgVpMZDOFAFIu7H9yhs%3D&st=2021-07-22T21%3A46%3A22Z&se=2021-07-23T05%3A56%3A22Z&sp=r', 'azureml-logs/65_job_prep-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/azureml-logs/65_job_prep-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt?sv=2019-02-02&sr=b&sig=YKzssT5rE7MDazKwPKoi2evRB9TlQfC7evcM2aqB0mk%3D&st=2021-07-22T21%3A46%3A22Z&se=2021-07-23T05%3A56%3A22Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=jAK6JlXcUH%2Ftn8eH0ySjic2WeoXcWoEDxxFCGhpJNM0%3D&st=2021-07-22T21%3A46%3A22Z&se=2021-07-23T05%3A56%3A22Z&sp=r', 'azureml-logs/75_job_post-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/azureml-logs/75_job_post-tvmps_f4a71e60693483258a16fd3a678c683668d1b65251846fa69bb84f00130897ed_d.txt?sv=2019-02-02&sr=b&sig=0uu7lztBQxk15ysDCZpL0Hv5JXg%2BBWGfHmHA%2FBMc514%3D&st=2021-07-22T21%3A46%3A22Z&se=2021-07-23T05%3A56%3A22Z&sp=r', 'azureml-logs/process_info.json': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=IBQfchdHGfl%2BlPm%2F0bNgjX7vX5H1PG3CGsKVNi2nW7o%3D&st=2021-07-22T21%3A46%3A22Z&se=2021-07-23T05%3A56%3A22Z&sp=r', 'azureml-logs/process_status.json': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=6YC78G5H56mVHhm8QkMLzkWMgSunOXavgWsD8CZwLL0%3D&st=2021-07-22T21%3A46%3A22Z&se=2021-07-23T05%3A56%3A22Z&sp=r', 'logs/azureml/96_azureml.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/logs/azureml/96_azureml.log?sv=2019-02-02&sr=b&sig=XoMu6Usieqwyjp4k97s6fSjMOgXubCZxD2apQYO7CWE%3D&st=2021-07-22T21%3A46%3A20Z&se=2021-07-23T05%3A56%3A20Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=LRHNPGAIh2y1xoKua%2B3C2AjEqpnKI8AFoF%2B%2BNNnd%2BaQ%3D&st=2021-07-22T21%3A46%3A20Z&se=2021-07-23T05%3A56%3A20Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=NVk1MPyeGOsk77usx4%2BCj%2FgyS13HiJAquemr7%2FhKXO0%3D&st=2021-07-22T21%3A46%3A20Z&se=2021-07-23T05%3A56%3A20Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=hOfgNg6N9lSiO8g6h1fx38vk0MKfFCiou6iLp2p6Jm0%3D&st=2021-07-22T21%3A46%3A20Z&se=2021-07-23T05%3A56%3A20Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=odF0ZNUwZQep9KGCAUf51LNRCg2bOSwUeCqn3CKyIzk%3D&st=2021-07-22T21%3A46%3A20Z&se=2021-07-23T05%3A56%3A20Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=prcAhIalhLP8OG6WdvHkJQ0FCt9eD87sIDkJrsGKYqY%3D&st=2021-07-22T21%3A46%3A20Z&se=2021-07-23T05%3A56%3A20Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=dhiV%2BC5Gtdx1wLUqIvvFthrgRKLhqBL1XINymR2ZXYk%3D&st=2021-07-22T21%3A46%3A20Z&se=2021-07-23T05%3A56%3A20Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.a0641745-601b-4995-83c8-a507c264b253/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=P1X5dLh6%2FJLCekVLyYsFVaBgDJTyp6XfAcK2f%2F%2FjsR0%3D&st=2021-07-22T21%3A46%3A20Z&se=2021-07-23T05%3A56%3A20Z&sp=r'}, 'submittedBy': 'Dennis Sawyers'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StepRunId: e6e55446-5f30-4c1b-a331-66a3b2f8826d\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/e6e55446-5f30-4c1b-a331-66a3b2f8826d?wsid=/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourcegroups/ml-teaching/workspaces/ml-teaching-workspace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( xgb-model-training-step-with-hyperparameter-tuning ) Status: Running\n",
      "\n",
      "StepRun(xgb-model-training-step-with-hyperparameter-tuning) Execution Summary\n",
      "==============================================================================\n",
      "StepRun( xgb-model-training-step-with-hyperparameter-tuning ) Status: Finished\n",
      "{'runId': 'e6e55446-5f30-4c1b-a331-66a3b2f8826d', 'status': 'Completed', 'startTimeUtc': '2021-07-22T21:56:32.108172Z', 'endTimeUtc': '2021-07-22T21:59:59.86589Z', 'properties': {'ContentSnapshotId': '262efdb6-ea36-48ee-ab1a-2df4fd2ba574', 'StepType': 'HyperDriveStep', 'ComputeTargetType': 'HyperDrive', 'azureml.moduleid': 'fa79cfb8-3a6e-476e-86d4-43232d1c17ba', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'cc2535e6', 'azureml.pipelinerunid': '83aec045-2a62-4d8a-bf04-6ee38a921cde'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.e6e55446-5f30-4c1b-a331-66a3b2f8826d/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=gMIy07vIblAQpeCBZ%2FDKLmFgDJW81sUklD4mg%2B1AP4k%3D&st=2021-07-22T21%3A46%3A34Z&se=2021-07-23T05%3A56%3A34Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.e6e55446-5f30-4c1b-a331-66a3b2f8826d/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=rfFeJD15%2B8qwBwSwMZmFRreXZVx2kLkbor11nNuj6S8%3D&st=2021-07-22T21%3A46%3A34Z&se=2021-07-23T05%3A56%3A34Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.e6e55446-5f30-4c1b-a331-66a3b2f8826d/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=zC0dqihYwz7GD4Dnj1Wmz0mrDpUE1gMpsKlYWnvwJdc%3D&st=2021-07-22T21%3A46%3A34Z&se=2021-07-23T05%3A56%3A34Z&sp=r'}, 'submittedBy': 'Dennis Sawyers'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StepRunId: faa45a95-fbf6-468c-a778-476909a81900\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/faa45a95-fbf6-468c-a778-476909a81900?wsid=/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourcegroups/ml-teaching/workspaces/ml-teaching-workspace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( model-registration-step ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt\n",
      "========================================================================================================================\n",
      "2021-07-22T22:00:12Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21928 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-07-22T22:00:13Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore\n",
      "2021-07-22T22:00:13Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T22:00:13Z Starting output-watcher...\n",
      "2021-07-22T22:00:13Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2021-07-22T22:00:13Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "2021-07-22T22:00:13Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_89938f9de59ccc23393c5d60d6d1f188\n",
      "Digest: sha256:2bfefaa91b273928de538ccf1e5abb5600366a8759874b3e3bad4b95159ae282\n",
      "Status: Image is up to date for mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188:latest\n",
      "mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188:latest\n",
      "2021-07-22T22:00:13Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T22:00:13Z Check if container faa45a95-fbf6-468c-a778-476909a81900 already exist exited with 0, \n",
      "\n",
      "c2dda6bcf0396f6c1ac0c771af7f4e8614d6e5794eb83db9a56a2ed89beb6398\n",
      "2021-07-22T22:00:14Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-07-22T22:00:14Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-5cff598d7cb6ada8ccac9b884df6e8fb-318abe6ffb047f9a-01 -sshRequired=false] \n",
      "2021/07/22 22:00:14 Starting App Insight Logger for task:  containerSetup\n",
      "2021/07/22 22:00:14 Version: 3.0.01650.0004 Branch: .SourceBranch Commit: 37e4354\n",
      "2021/07/22 22:00:14 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/07/22 22:00:14 Starting infiniband setup\n",
      "2021/07/22 22:00:14 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/07/22 22:00:14 Returning Python Version as 3.6\n",
      "2021-07-22T22:00:14Z VMSize: standard_ds3_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/07/22 22:00:14 VMSize: standard_ds3_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/07/22 22:00:14 VMSize: standard_ds3_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/07/22 22:00:14 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021-07-22T22:00:14Z Not setting up Infiniband in Container\n",
      "2021/07/22 22:00:14 Not setting up Infiniband in Container\n",
      "2021/07/22 22:00:14 Not setting up Infiniband in Container\n",
      "2021/07/22 22:00:14 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/07/22 22:00:14 Returning Python Version as 3.6\n",
      "2021/07/22 22:00:14 sshd inside container not required for job, skipping setup.\n",
      "2021/07/22 22:00:14 All App Insights Logs was sent successfully or the close timeout of 20 was reached\n",
      "2021/07/22 22:00:14 App Insight Client has already been closed\n",
      "2021/07/22 22:00:14 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-07-22T22:00:14Z Starting docker container succeeded.\n",
      "2021-07-22T22:00:19Z Job environment preparation succeeded on 10.0.0.4. Output: \n",
      ">>>   2021/07/22 22:00:12 Starting App Insight Logger for task:  prepareJobEnvironment\n",
      ">>>   2021/07/22 22:00:12 Version: 3.0.01650.0004 Branch: .SourceBranch Commit: 37e4354\n",
      ">>>   2021/07/22 22:00:12 runtime.GOOS linux\n",
      ">>>   2021/07/22 22:00:12 Checking if '/tmp' exists\n",
      ">>>   2021/07/22 22:00:12 Reading dyanamic configs\n",
      ">>>   2021/07/22 22:00:12 Container sas url: https://baiscriptsdm1prod.blob.core.windows.net/aihosttools?sv=2018-03-28&sr=c&si=aihosttoolspolicy&sig=0dYPjOcglC72KKRE2ILzFCKnPug7ECc2SIRWA3udLW0%3D\n",
      ">>>   2021/07/22 22:00:12 Failed to read from file /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables, open /mnt/batch/tasks/startup/wd/az_resource/xdsenv.variable/azsecpack.variables: no such file or directory\n",
      ">>>   2021/07/22 22:00:12 [in autoUpgradeFromJobNodeSetup] Is Azsecpack installer on host: false. Is Azsecpack installation enabled: false,\n",
      ">>>   2021/07/22 22:00:12 Starting Azsecpack installation on machine: 7283217cffb4431c9c10e46428534852000000#72f988bf-86f1-41af-91ab-2d7cd011db47#47a7ec0c-37ad-428b-9114-b87ea1057632#ml-teaching#ml-teaching-workspace#automl-cluster#tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d\n",
      ">>>   2021/07/22 22:00:12 Is Azsecpack enabled: false, GetDisableVsatlsscan: true\n",
      ">>>   2021/07/22 22:00:12 Turning off azsecpack, if it is already running\n",
      ">>>   2021/07/22 22:00:12 [doTurnOffAzsecpack] output:Unit mdsd.service could not be found.\n",
      ">>>   ,err:exit status 1.\n",
      ">>>   2021/07/22 22:00:12 OS patching disabled by dynamic configs. Skipping.\n",
      ">>>   2021/07/22 22:00:12 DetonationChamber is not enabled on this subscription: 47a7ec0c-37ad-428b-9114-b87ea1057632\n",
      ">>>   2021/07/22 22:00:12 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 22:00:12 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 22:00:12 Get GPU count failed with err: The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command., \n",
      ">>>   2021/07/22 22:00:12 AMLComputeXDSEndpoint:  https://dm1-prodk8ds.batchai.core.windows.net\n",
      ">>>   2021/07/22 22:00:12 AMLComputeXDSApiVersion:  2018-02-01\n",
      ">>>   2021/07/22 22:00:12 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/config\n",
      ">>>   2021/07/22 22:00:12 This is not a aml-workstation (compute instance), current offer type: amlcompute. Starting identity responder as part of prepareJobEnvironment.\n",
      ">>>   2021/07/22 22:00:12 Starting identity responder.\n",
      ">>>   2021/07/22 22:00:12 Starting identity responder.\n",
      ">>>   2021/07/22 22:00:12 Failed to open file /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/config/.batchai.IdentityResponder.envlist: open /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/config/.batchai.IdentityResponder.envlist: no such file or directory\n",
      ">>>   2021/07/22 22:00:12 Logfile used for identity responder: /mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/IdentityResponderLog-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt\n",
      ">>>   2021/07/22 22:00:12 Logfile used for identity responder: /mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/IdentityResponderLog-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt\n",
      ">>>   2021/07/22 22:00:12 Started Identity Responder for job.\n",
      ">>>   2021/07/22 22:00:12 Started Identity Responder for job.\n",
      ">>>   2021/07/22 22:00:12 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd\n",
      ">>>   2021/07/22 22:00:12 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/shared\n",
      ">>>   2021/07/22 22:00:12 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/07/22 22:00:12 Mounting job level file systems\n",
      ">>>   2021/07/22 22:00:12 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts\n",
      ">>>   2021/07/22 22:00:12 Attempting to read datastore credentials file: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/config/.amlcompute.datastorecredentials\n",
      ">>>   2021/07/22 22:00:12 Datastore credentials file not found, skipping.\n",
      ">>>   2021/07/22 22:00:12 Attempting to read runtime sas tokens file: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/config/.master.runtimesastokens\n",
      ">>>   2021/07/22 22:00:12 Runtime sas tokens file not found, skipping.\n",
      ">>>   2021/07/22 22:00:12 No NFS configured\n",
      ">>>   2021/07/22 22:00:12 No Azure File Shares configured\n",
      ">>>   2021/07/22 22:00:12 Mounting blob file systems\n",
      ">>>   2021/07/22 22:00:12 Blobfuse runtime version 1.3.6\n",
      ">>>   2021/07/22 22:00:12 Mounting azureml-blobstore-d8c754f2-1214-460a-b696-a73d7a887cd1 container from mlteachingwork0962888719 account at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore\n",
      ">>>   2021/07/22 22:00:12 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/07/22 22:00:12 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/07/22 22:00:12 Blobfuse cache size set to 21928 MB.\n",
      ">>>   2021/07/22 22:00:12 Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21928 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      ">>>   2021/07/22 22:00:13 Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore\n",
      ">>>   2021/07/22 22:00:13 Waiting for blobfs to be mounted at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore\n",
      ">>>   2021/07/22 22:00:13 Successfully mounted azureml-blobstore-d8c754f2-1214-460a-b696-a73d7a887cd1 container from mlteachingwork0962888719 account at /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore\n",
      ">>>   2021/07/22 22:00:13 Created run_id directory: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/faa45a95-fbf6-468c-a778-476909a81900\n",
      ">>>   2021/07/22 22:00:13 No unmanaged file systems configured\n",
      ">>>   2021/07/22 22:00:13 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 22:00:13 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 22:00:13 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/07/22 22:00:13 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/azureml_compute_logs\n",
      ">>>   2021/07/22 22:00:13 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/logs\n",
      ">>>   2021/07/22 22:00:13 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/outputs\n",
      ">>>   2021/07/22 22:00:13 Starting output-watcher...\n",
      ">>>   2021/07/22 22:00:13 Single file input dataset is enabled.\n",
      ">>>   2021/07/22 22:00:13 Start to pulling docker image: mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188\n",
      ">>>   2021/07/22 22:00:13 Start pull docker image: mlteachingwoe1b28e33.azurecr.io\n",
      ">>>   2021/07/22 22:00:13 Getting credentials for image mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188 with url mlteachingwoe1b28e33.azurecr.io\n",
      ">>>   2021/07/22 22:00:13 Container registry is ACR.\n",
      ">>>   2021/07/22 22:00:13 Skip getting ACR Credentials from Identity and will be getting it from EMS\n",
      ">>>   2021/07/22 22:00:13 Getting ACR Credentials from EMS for environment XGBoostTrainingEnv:Autosave_2021-07-20T06:10:19Z_4298b406\n",
      ">>>   2021/07/22 22:00:13 Requesting XDS for registry details.\n",
      ">>>   2021/07/22 22:00:13 Attempt 1 of http call to https://dm1-prodk8ds.batchai.core.windows.net/hosttoolapi/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourceGroups/ml-teaching/workspaces/ml-teaching-workspace/clusters/automl-cluster/nodes/tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d?api-version=2018-02-01\n",
      ">>>   2021/07/22 22:00:13 Got container registry details from credentials service for registry address: mlteachingwoe1b28e33.azurecr.io.\n",
      ">>>   2021/07/22 22:00:13 Writing ACR Details to file...\n",
      ">>>   2021/07/22 22:00:13 Copying ACR Details file to worker nodes...\n",
      ">>>   2021/07/22 22:00:13 Executing 'Copy ACR Details file' on 10.0.0.4\n",
      ">>>   2021/07/22 22:00:13 Begin executing 'Copy ACR Details file' task on Node\n",
      ">>>   2021/07/22 22:00:13 'Copy ACR Details file' task Node result: succeeded\n",
      ">>>   2021/07/22 22:00:13 Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   >>>   \n",
      ">>>   >>>   \n",
      ">>>   2021/07/22 22:00:13 Successfully retrieved ACR Credentials from EMS.\n",
      ">>>   2021/07/22 22:00:13 EMS returned mlteachingwoe1b28e33.azurecr.io for environment XGBoostTrainingEnv\n",
      ">>>   2021/07/22 22:00:13 Save docker credentials for image mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188 in /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/docker_login_583FB053A9D31AE7\n",
      ">>>   2021/07/22 22:00:13 Start login to the docker registry\n",
      ">>>   2021/07/22 22:00:13 Successfully logged into the docker registry.\n",
      ">>>   2021/07/22 22:00:13 Start run pull docker image command\n",
      ">>>   2021/07/22 22:00:13 Pull docker image succeeded.\n",
      ">>>   2021/07/22 22:00:13 Removed docker config dir /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/docker_login_583FB053A9D31AE7\n",
      ">>>   2021/07/22 22:00:13 Pull docker image time: 304.477301ms\n",
      ">>>   \n",
      ">>>   2021/07/22 22:00:13 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/07/22 22:00:13 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 22:00:13 The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/07/22 22:00:13 Setting the memory limit for docker container to be 13674 MB\n",
      ">>>   2021/07/22 22:00:13 The env variable file size is 24529 bytes\n",
      ">>>   2021/07/22 22:00:13 Creating parent cgroup 'faa45a95-fbf6-468c-a778-476909a81900' for Containers used in Job\n",
      ">>>   2021/07/22 22:00:13 Add parent cgroup 'faa45a95-fbf6-468c-a778-476909a81900' to container 'faa45a95-fbf6-468c-a778-476909a81900'\n",
      ">>>   2021/07/22 22:00:13 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      ">>>   2021/07/22 22:00:13 Original Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,faa45a95-fbf6-468c-a778-476909a81900,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/certs:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/certs,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-m,13674m,-v,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/azureml_compute_logs,-v,/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/wd:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/wd,-v,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900,-v,/mnt/batch/tasks/shared/LS_root/shared/tracing/faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/tracing,-w,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/config/.batchai.envlist,--cgroup-parent=/faa45a95-fbf6-468c-a778-476909a81900/,--shm-size,2g\n",
      ">>>   2021/07/22 22:00:13 the binding /mnt/batch/tasks/shared/LS_root/shared/tracing/faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/tracing is discarded as we already have /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared \n",
      ">>>   2021/07/22 22:00:13 the binding /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/azureml_compute_logs is discarded as we already have /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900 \n",
      ">>>   2021/07/22 22:00:13 Updated Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,faa45a95-fbf6-468c-a778-476909a81900,-m,13674m,-w,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/config/.batchai.envlist,--cgroup-parent=/faa45a95-fbf6-468c-a778-476909a81900/,--shm-size,2g,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900,-v,/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/wd:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/wd,-v,/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/certs:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/certs\n",
      ">>>   2021/07/22 22:00:13 Running Docker command: docker run --ulimit memlock=9223372036854775807 --ulimit nofile=262144:262144 --cap-add sys_ptrace --name faa45a95-fbf6-468c-a778-476909a81900 -m 13674m -w /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd --expose 23 --env-file /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/config/.batchai.envlist --cgroup-parent=/faa45a95-fbf6-468c-a778-476909a81900/ --shm-size 2g -v /mnt/batch/tasks/startup:/mnt/batch/tasks/startup -v /mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared -v /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared -v /mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs -v /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900:/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900 -v /mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/wd:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/wd -v /mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/certs:/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/certs -d -it --privileged --net=host mlteachingwoe1b28e33.azurecr.io/azureml/azureml_89938f9de59ccc23393c5d60d6d1f188\n",
      ">>>   2021/07/22 22:00:13 Check if container faa45a95-fbf6-468c-a778-476909a81900 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/07/22 22:00:13 Check if container faa45a95-fbf6-468c-a778-476909a81900 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/07/22 22:00:14 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/07/22 22:00:14 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/07/22 22:00:14 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-5cff598d7cb6ada8ccac9b884df6e8fb-318abe6ffb047f9a-01 -sshRequired=false] \n",
      ">>>   2021/07/22 22:00:14 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-5cff598d7cb6ada8ccac9b884df6e8fb-318abe6ffb047f9a-01 -sshRequired=false] \n",
      ">>>   2021/07/22 22:00:14 Container ssh is not required for job type.\n",
      ">>>   2021/07/22 22:00:14 Starting docker container succeeded.\n",
      ">>>   2021/07/22 22:00:14 Starting docker container succeeded.\n",
      ">>>   2021/07/22 22:00:14 Disk space after starting docker container: 23381MB\n",
      ">>>   2021/07/22 22:00:14 Begin execution of runSpecialJobTask\n",
      ">>>   2021/07/22 22:00:14 runSpecialJobTask: os.GetEnv constants.StdouterrDir: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/azureml_compute_logs\n",
      ">>>   2021/07/22 22:00:14 runSpecialJobTask: Raw cmd for preparation is passed is: /azureml-envs/azureml_d6837a920ac9d0a042e4b8fb48440fae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/faa45a95-fbf6-468c-a778-476909a81900-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"262efdb6-ea36-48ee-ab1a-2df4fd2ba574\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/07/22 22:00:14 runSpecialJobTask: stdout path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/azureml_compute_logs/65_job_prep-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt\n",
      ">>>   2021/07/22 22:00:14 runSpecialJobTask: stderr path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900/azureml_compute_logs/65_job_prep-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt\n",
      ">>>   2021/07/22 22:00:14 native cmd: export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900;/azureml-envs/azureml_d6837a920ac9d0a042e4b8fb48440fae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/faa45a95-fbf6-468c-a778-476909a81900-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"262efdb6-ea36-48ee-ab1a-2df4fd2ba574\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/07/22 22:00:14 runSpecialJobTask: commons.GetOsPlatform(): ubuntu\n",
      ">>>   2021/07/22 22:00:14 runSpecialJobTask: Running cmd: /usr/bin/docker exec -e AZUREML_SDK_TRACEPARENT=00-5cff598d7cb6ada8ccac9b884df6e8fb-0c08c72e5c8b3562-01 -t faa45a95-fbf6-468c-a778-476909a81900 bash -c if [ -f ~/.bashrc ]; then PS1_back=$PS1; PS1='$'; . ~/.bashrc; PS1=$PS1_back; fi;PATH=$PATH:$AZ_BATCH_NODE_STARTUP_DIR/wd/;export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/0c257b9c-0eaa-4b8d-b2e5-a1b16f371324/job-1/faa45a95-fbf6-468c-a_25593440-f0a1-4938-8032-363175e89be9/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900;/azureml-envs/azureml_d6837a920ac9d0a042e4b8fb48440fae/bin/python /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/faa45a95-fbf6-468c-a778-476909a81900-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"262efdb6-ea36-48ee-ab1a-2df4fd2ba574\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/07/22 22:00:17 Attempt 1 of http call to https://centralus.api.azureml.ms/history/v1.0/private/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourceGroups/ml-teaching/providers/Microsoft.MachineLearningServices/workspaces/ml-teaching-workspace/runs/faa45a95-fbf6-468c-a778-476909a81900/spans\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: job preparation exited with code 0 and err <nil>\n",
      ">>>   \n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:15.360427] Entering job preparation.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.142639] Starting job preparation.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.142683] Extracting the control code.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.143097] Starting extract_project.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.143145] Starting to extract zip file.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.163626] Finished extracting zip file.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.175314] Using urllib.request Python 3.0 or later\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.175357] Start fetching snapshots.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.175387] Start fetching snapshot.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.175398] Retrieving project from snapshot: 262efdb6-ea36-48ee-ab1a-2df4fd2ba574\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: Starting the daemon thread to refresh tokens in background for process with pid = 43\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.760382] Finished fetching snapshot.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.760412] Finished fetching snapshots.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.760427] Finished extract_project.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.760518] Finished fetching and extracting the control code.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.764401] downloadDataStore - Download from datastores if requested.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.768756] Start run_history_prep.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:17.770883] Entering context manager injector.\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: Acquired lockfile /tmp/faa45a95-fbf6-468c-a778-476909a81900-datastore.lock to downloading input data references\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:18.276246] downloadDataStore completed\n",
      ">>>   2021/07/22 22:00:18 runSpecialJobTask: preparation: [2021-07-22T22:00:18.279410] Job preparation is complete.\n",
      ">>>   2021/07/22 22:00:18 Execution of runSpecialJobTask completed\n",
      ">>>   2021/07/22 22:00:18 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      ">>>   Stopped: false\n",
      ">>>   OriginalData: 3\n",
      ">>>   FilteredData: 0.\n",
      ">>>   2021/07/22 22:00:18 Process Exiting with Code:  0\n",
      ">>>   2021/07/22 22:00:19 All App Insights Logs was sent successfully or the close timeout of 20 was reached\n",
      ">>>   \n",
      "2021-07-22T22:00:19Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T22:00:19Z The vmsize standard_ds3_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-07-22T22:00:19Z 127.0.0.1 slots=4 max-slots=4\n",
      "2021-07-22T22:00:19Z launching Custom job\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "2021/07/22 22:00:19 Starting App Insight Logger for task:  runTaskLet\n",
      "2021/07/22 22:00:19 Version: 3.0.01650.0004 Branch: .SourceBranch Commit: 37e4354\n",
      "2021/07/22 22:00:19 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info\n",
      "2021/07/22 22:00:19 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\n",
      "[2021-07-22T22:00:19.506534] Entering context manager injector.\n",
      "[2021-07-22T22:00:20.010608] context_manager_injector.py Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['XGB_Hyperdrive_Model_Registration.py', '--train_dataset_name', 'XGB Training Data', '--val_dataset_name', 'XGB Validation Data', '--datastore_name', 'ds_dev', '--project_name', 'XGB Test', '--project_description', 'XGB Test Run', '--pytz_time_zone', 'US/Eastern', '--target_column_name', 'LeftCompany', '--k_folds', '10', '--confidence_level', '0.95', '--model_name', 'Tuned-XGB-Model', '--output_path', 'XGB/XGB_Training_Output', '--scoring_metric', 'Balanced Accuracy Training', '--metric_goal', 'MAXIMIZE', '--saved_model', '/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/saved_model', '--explainer_model', '/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/explainer_model', '--metrics_data', '/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/metrics_data'])\n",
      "Script type = None\n",
      "[2021-07-22T22:00:20.014251] Entering Run History Context Manager.\n",
      "[2021-07-22T22:00:20.750096] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/wd/azureml/faa45a95-fbf6-468c-a778-476909a81900\n",
      "[2021-07-22T22:00:20.750385] Preparing to call script [XGB_Hyperdrive_Model_Registration.py] with arguments:['--train_dataset_name', 'XGB Training Data', '--val_dataset_name', 'XGB Validation Data', '--datastore_name', 'ds_dev', '--project_name', 'XGB Test', '--project_description', 'XGB Test Run', '--pytz_time_zone', 'US/Eastern', '--target_column_name', 'LeftCompany', '--k_folds', '10', '--confidence_level', '0.95', '--model_name', 'Tuned-XGB-Model', '--output_path', 'XGB/XGB_Training_Output', '--scoring_metric', 'Balanced Accuracy Training', '--metric_goal', 'MAXIMIZE', '--saved_model', '/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/saved_model', '--explainer_model', '/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/explainer_model', '--metrics_data', '/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/metrics_data']\n",
      "[2021-07-22T22:00:20.750438] After variable expansion, calling script [XGB_Hyperdrive_Model_Registration.py] with arguments:['--train_dataset_name', 'XGB Training Data', '--val_dataset_name', 'XGB Validation Data', '--datastore_name', 'ds_dev', '--project_name', 'XGB Test', '--project_description', 'XGB Test Run', '--pytz_time_zone', 'US/Eastern', '--target_column_name', 'LeftCompany', '--k_folds', '10', '--confidence_level', '0.95', '--model_name', 'Tuned-XGB-Model', '--output_path', 'XGB/XGB_Training_Output', '--scoring_metric', 'Balanced Accuracy Training', '--metric_goal', 'MAXIMIZE', '--saved_model', '/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/saved_model', '--explainer_model', '/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/explainer_model', '--metrics_data', '/mnt/batch/tasks/shared/LS_root/jobs/ml-teaching-workspace/azureml/faa45a95-fbf6-468c-a778-476909a81900/mounts/workspaceblobstore/azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/metrics_data']\n",
      "\n",
      "Script Initialized\n",
      "Scoring Metric Set\n",
      "Metrics File Downloaded\n",
      "Metrics Dataframe Created\n",
      "Datastore Set\n",
      "2021/07/22 22:00:24 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "Datasets Retrieved\n",
      "Datasets Converted to Pandas\n",
      "Validation Data split into Feature and Target Columns\n",
      "Training Model Loaded\n",
      "Explainer Model Loaded\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "Explanations Saved to Validation Data\n",
      "Global Explanations Saved as Pandas Dataframe\n",
      "Validation Predictions written to CSV file in logs\n",
      "Time Zone Set\n",
      "Output Directory Created\n",
      "Data Written as CSV and saved to XGB/XGB_Training_Output/2021-07-22/validationPredictions.csv\n",
      "Uploading an estimated of 1 files\n",
      "Uploading XGB/XGB_Training_Output/2021-07-22/validationPredictions.csv\n",
      "Uploaded XGB/XGB_Training_Output/2021-07-22/validationPredictions.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Predictions with Explanations for Validation Data Loaded to Datastore\n",
      "Data Written as CSV and saved to XGB/XGB_Training_Output/2021-07-22/globalExplanations.csv\n",
      "Uploading an estimated of 1 files\n",
      "Uploading XGB/XGB_Training_Output/2021-07-22/globalExplanations.csv\n",
      "Uploaded XGB/XGB_Training_Output/2021-07-22/globalExplanations.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Global Expanations Loaded to Datastore\n",
      "Predictions Made for Validation Data\n",
      "Validation Set Balanced Accuracy Training is 0.93\n",
      "Model Balanced Accuracy Training is 95.0% likely to actually fall between [0.8089522423605693] and [0.827113055368927]\n",
      "Model is performing as expected on validation data.\n",
      "Models Being Compared\n",
      "Previous Model Loaded and is being Compared to New Model\n",
      "Model Tags and Description Assigned\n",
      "Old model outperforms new model and new model will not be registered.\n",
      "Script Finished\n",
      "\n",
      "\n",
      "[2021-07-22T22:00:38.816368] The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
      "17 items cleaning up...\n",
      "Cleanup took 1.180293321609497 seconds\n",
      "[2021-07-22T22:00:40.123375] Finished context manager injector.\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt\n",
      "===============================================================================================================\n",
      "[2021-07-22T22:00:47.114193] Entering job release\n",
      "[2021-07-22T22:00:48.097012] Starting job release\n",
      "[2021-07-22T22:00:48.097511] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 444\n",
      "[2021-07-22T22:00:48.097788] job release stage : upload_datastore starting...\n",
      "[2021-07-22T22:00:48.098316] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-07-22T22:00:48.098631] Entering context manager injector.\n",
      "[2021-07-22T22:00:48.100789] job release stage : execute_job_release starting...\n",
      "[2021-07-22T22:00:48.101229] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-07-22T22:00:48.101279] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-07-22T22:00:48.132520] job release stage : upload_datastore completed...\n",
      "[2021-07-22T22:00:48.203101] job release stage : send_run_telemetry starting...\n",
      "[2021-07-22T22:00:48.249274] get vm size and vm region successfully.\n",
      "[2021-07-22T22:00:48.254657] get compute meta data successfully.\n",
      "Failed to upload compute record artifact, error_details=module 'azureml_globals' has no attribute 'compute_rcord_artifact_path'\n",
      "[2021-07-22T22:00:48.254733] upload compute record artifact successfully.\n",
      "[2021-07-22T22:00:48.254838] job release stage : send_run_telemetry completed...\n",
      "[2021-07-22T22:00:48.322612] job release stage : execute_job_release completed...\n",
      "[2021-07-22T22:00:48.322815] Job release is complete\n",
      "\n",
      "StepRun(model-registration-step) Execution Summary\n",
      "===================================================\n",
      "StepRun( model-registration-step ) Status: Finished\n",
      "{'runId': 'faa45a95-fbf6-468c-a778-476909a81900', 'target': 'automl-cluster', 'status': 'Completed', 'startTimeUtc': '2021-07-22T22:00:11.764848Z', 'endTimeUtc': '2021-07-22T22:00:58.802779Z', 'properties': {'ContentSnapshotId': '262efdb6-ea36-48ee-ab1a-2df4fd2ba574', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '1bec9d14-830f-41c0-baf5-0319c8d09cab', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '5c64d6b9', 'azureml.pipelinerunid': '83aec045-2a62-4d8a-bf04-6ee38a921cde', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'azureml.RuntimeType': 'Hosttools'}, 'inputDatasets': [{'dataset': {'id': '235da23e-b7b7-4c51-9dbd-7f53a55e3542'}, 'consumptionDetails': {'type': 'Reference'}}, {'dataset': {'id': '73a3da00-3023-4c53-9599-2b50c6726b93'}, 'consumptionDetails': {'type': 'Reference'}}], 'outputDatasets': [], 'runDefinition': {'script': 'XGB_Hyperdrive_Model_Registration.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--train_dataset_name', '$AML_PARAMETER_TrainDatasetName', '--val_dataset_name', '$AML_PARAMETER_ValDatasetName', '--datastore_name', '$AML_PARAMETER_DatastoreName', '--project_name', '$AML_PARAMETER_ProjectName', '--project_description', '$AML_PARAMETER_ProjectDescription', '--pytz_time_zone', '$AML_PARAMETER_PytzTimeZone', '--target_column_name', '$AML_PARAMETER_TargetColumn', '--k_folds', '$AML_PARAMETER_KFolds', '--confidence_level', '$AML_PARAMETER_ConfidenceLevel', '--model_name', '$AML_PARAMETER_ModelName', '--output_path', '$AML_PARAMETER_OutputPath', '--scoring_metric', '$AML_PARAMETER_ScoringMetric', '--metric_goal', '$AML_PARAMETER_MetricGoal', '--saved_model', '$AZUREML_DATAREFERENCE_saved_model', '--explainer_model', '$AZUREML_DATAREFERENCE_explainer_model', '--metrics_data', '$AZUREML_DATAREFERENCE_metrics_data'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'automl-cluster', 'dataReferences': {'saved_model': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/saved_model', 'pathOnCompute': None, 'overwrite': False}, 'explainer_model': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/explainer_model', 'pathOnCompute': None, 'overwrite': False}, 'metrics_data': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/e6e55446-5f30-4c1b-a331-66a3b2f8826d/metrics_data', 'pathOnCompute': None, 'overwrite': False}}, 'data': {}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'XGBoostTrainingEnv', 'version': 'Autosave_2021-07-20T06:10:19Z_4298b406', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults==1.31.0', 'azureml-interpret==1.31.0', 'azureml-explain-model==1.31.0', 'pyarrow==1.0.1', 'pytz==2021.1', 'interpret-core==0.1.21', 'lightgbm==2.3.0']}, 'scikit-learn==0.22.1', 'numpy==1.16.2', 'matplotlib==3.2.1', 'joblib==0.14.1', 'xgboost==0.90', 'seaborn==0.9.0', 'pandas==0.23.4', 'scipy==1.3.1'], 'name': 'azureml_d6837a920ac9d0a042e4b8fb48440fae'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AML_PARAMETER_TrainDatasetName': 'XGB Training Data', 'AML_PARAMETER_ValDatasetName': 'XGB Validation Data', 'AML_PARAMETER_DatastoreName': 'ds_dev', 'AML_PARAMETER_ProjectName': 'XGB Test', 'AML_PARAMETER_ProjectDescription': 'XGB Test Run', 'AML_PARAMETER_PytzTimeZone': 'US/Eastern', 'AML_PARAMETER_TargetColumn': 'LeftCompany', 'AML_PARAMETER_KFolds': '10', 'AML_PARAMETER_ConfidenceLevel': '0.95', 'AML_PARAMETER_ModelName': 'Tuned-XGB-Model', 'AML_PARAMETER_OutputPath': 'XGB/XGB_Training_Output', 'AML_PARAMETER_ScoringMetric': 'Balanced Accuracy Training', 'AML_PARAMETER_MetricGoal': 'MAXIMIZE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210531.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'enableAzmlInt': True, 'priority': None, 'slaTier': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/azureml-logs/55_azureml-execution-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt?sv=2019-02-02&sr=b&sig=pXPLQJsk0kSFP15T8AO4cJeMkC258719Q3X%2Fc9ZuSgs%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'azureml-logs/65_job_prep-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/azureml-logs/65_job_prep-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt?sv=2019-02-02&sr=b&sig=X5MU9hjcwtKkURuZgbxP5lQo%2FUtkiaz7md9tUK4MWhQ%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=eJ%2BT03Qe3lTKt8Cp7cmyaXYMaUfrnta81Zrm1oOcMU0%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'azureml-logs/75_job_post-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/azureml-logs/75_job_post-tvmps_82277007c4b0b4d9dc7bca373984c896bc6403cf4913934bfa4695f14fa72b23_d.txt?sv=2019-02-02&sr=b&sig=MQ2oboywQgpj2kbJV1Kt0%2FquqkFqoGsK18wuESdXSvg%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'azureml-logs/process_info.json': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=pHJoG20b1FhoTvk7muABTZnTmf%2ByfcgPBlEn%2BHVUoKk%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'azureml-logs/process_status.json': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=HlkP5V0Y1qpKAoDRvTL3V7Db7vezRVlVNOt01uZ7WNs%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'logs/azureml/97_azureml.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/97_azureml.log?sv=2019-02-02&sr=b&sig=nqkfLCG1wcXqU32ZFbcNtvkZ085vQ21wQWSxNap0AgQ%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=RahjiUjdONuz0UHXD6jDmGnEN3KkcJc%2FUB3Woe8o3gs%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=FIYotjsNHad%2FHY4%2BZbs%2FH5La4OSW04lmPfwJi6WWQjY%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=hgR2%2B8exA8%2FG9HhVamksDHY%2F4AKWsWVjZboCeVXyCLg%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=P2qXAqQq3y3fVYZTBUCBdAy53Em3p6BZyqIsq2rnFII%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=vTU%2Fha%2BJNkWGoQ4hm0HPLLV9LUQwoASNvAvIfUwBeik%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=uCZd0Ciqbt70LLXJUmDLqXkElq%2BCCd0GbxEDLYT8kDM%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.faa45a95-fbf6-468c-a778-476909a81900/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=GCK6tlnoSZJi%2FgtDJm3GGa1idHR3KF6J2BVASWD3Rb0%3D&st=2021-07-22T21%3A50%3A50Z&se=2021-07-23T06%3A00%3A50Z&sp=r'}, 'submittedBy': 'Dennis Sawyers'}\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '83aec045-2a62-4d8a-bf04-6ee38a921cde', 'status': 'Completed', 'startTimeUtc': '2021-07-22T21:55:09.380811Z', 'endTimeUtc': '2021-07-22T22:00:59.980023Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{\"TrainDatasetName\":\"XGB Training Data\",\"ValDatasetName\":\"XGB Validation Data\",\"DatastoreName\":\"ds_dev\",\"DatastorePath\":\"XGB/XGB_Training_Input\",\"TrainFileName\":\"xgbTrainingData.csv\",\"ValFileName\":\"xgbValidationData.csv\",\"ProjectName\":\"XGB Test\",\"ProjectDescription\":\"XGB Test Run\",\"PytzTimeZone\":\"US/Eastern\",\"TargetColumn\":\"LeftCompany\",\"KFolds\":\"10\",\"ShuffleSplitSize\":\"0.1\",\"ConfidenceLevel\":\"0.95\",\"ModelName\":\"Tuned-XGB-Model\",\"OutputPath\":\"XGB/XGB_Training_Output\",\"ScoringMetric\":\"Balanced Accuracy Training\",\"MetricGoal\":\"MAXIMIZE\"}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.83aec045-2a62-4d8a-bf04-6ee38a921cde/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=J1fssT3ssFDxtStyDVFBkmZXG0kUzwEx%2FC%2B40bQV%2FTY%3D&st=2021-07-22T21%3A45%3A12Z&se=2021-07-23T05%3A55%3A12Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.83aec045-2a62-4d8a-bf04-6ee38a921cde/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=TNe60QEMtwDL0o2P2mEPmOuK0tX2tLUFB9KZM82ELlM%3D&st=2021-07-22T21%3A45%3A12Z&se=2021-07-23T05%3A55%3A12Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mlteachingwork0962888719.blob.core.windows.net/azureml/ExperimentRun/dcid.83aec045-2a62-4d8a-bf04-6ee38a921cde/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=CKaWGb0eMugUfXqqCW91ULVFFrvoZi6BRl2gutCrTgI%3D&st=2021-07-22T21%3A45%3A12Z&se=2021-07-23T05%3A55%3A12Z&sp=r'}, 'submittedBy': 'Dennis Sawyers'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56747663b3284e5a9e65dbddc302923b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GUI to see your Pipeline Run\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish your Pipeline\n",
    "First, if you shutdown your notebook, use the first cell to retrieve your pipeline run.\n",
    "\n",
    "Second, publish your pipeline. \n",
    "\n",
    "Third, assign your published pipeline to a permanent endpoint.  \n",
    "\n",
    "You now have an endpoint you can easily schedule either in AMLS or through <b>Azure Data Factory</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a previously run pipeline if necessary by uncommenting and running the code below\n",
    "#experiment_name = 'XGB_Model_Training'\n",
    "#experiment = Experiment(ws, experiment_name)\n",
    "#pipeline_run = PipelineRun(experiment, 'your-pipeline-run-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>XGB_Model_Training</td><td><a href=\"https://ml.azure.com/pipelines/ff43c875-d9ee-407c-83f2-b5235e3b5d2d?wsid=/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourcegroups/ml-teaching/workspaces/ml-teaching-workspace\" target=\"_blank\" rel=\"noopener\">ff43c875-d9ee-407c-83f2-b5235e3b5d2d</a></td><td>Active</td><td><a href=\"https://centralus.api.azureml.ms/pipelines/v1.0/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourceGroups/ml-teaching/providers/Microsoft.MachineLearningServices/workspaces/ml-teaching-workspace/PipelineRuns/PipelineSubmit/ff43c875-d9ee-407c-83f2-b5235e3b5d2d\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: XGB_Model_Training,\n",
       "Id: ff43c875-d9ee-407c-83f2-b5235e3b5d2d,\n",
       "Status: Active,\n",
       "Endpoint: https://centralus.api.azureml.ms/pipelines/v1.0/subscriptions/47a7ec0c-37ad-428b-9114-b87ea1057632/resourceGroups/ml-teaching/providers/Microsoft.MachineLearningServices/workspaces/ml-teaching-workspace/PipelineRuns/PipelineSubmit/ff43c875-d9ee-407c-83f2-b5235e3b5d2d)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Publish your Pipeline\n",
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"XGB_Model_Training\",\\\n",
    "    description=\"XGB Model Training Pipeline for ADF Use\", version=\"1.0\")\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach your Published Pipeline to a Permanent Endpoint\n",
    "pipelineEndpointName = \"XGB Training Pipeline Endpoint\"\n",
    "\n",
    "if pipelineEndpointName in str(PipelineEndpoint.list(ws)):\n",
    "    # Add a new Version to an existing Endpoint\n",
    "    pipeline_endpoint = PipelineEndpoint.get(workspace = ws, name = pipelineEndpointName)\n",
    "    pipeline_endpoint.add_default(published_pipeline)\n",
    "else:\n",
    "    # Create a new Endpoint\n",
    "    pipeline_endpoint = PipelineEndpoint.publish(workspace = ws,\n",
    "                                                name = pipelineEndpointName,\n",
    "                                                pipeline = published_pipeline,\n",
    "                                                description = \"XGB Training Pipeline Endpoint\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
